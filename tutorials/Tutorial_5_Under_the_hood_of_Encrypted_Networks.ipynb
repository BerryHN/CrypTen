{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll take a look at how CrypTen performs inference with an encrypted neural network on encrypted data. We'll see how the data remains encrypted through all the operations, and yet is able to obtain accurate results after the computation. \n",
    "\n",
    "\n",
    "### A Simple Linear Layer\n",
    "We'll start by examining how a single Linear layer works in CrypTen. We'll instantiate a Linear layer, encrypt it, and step through some toy data with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:DistributedCommunicator with rank 0\n",
      "INFO:root:==================\n",
      "INFO:root:World size = 1\n"
     ]
    }
   ],
   "source": [
    "import crypten\n",
    "import torch\n",
    "\n",
    "#generate some toy data\n",
    "features = 4\n",
    "examples = 3\n",
    "toy_data = torch.rand(examples, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plaintext Weights: Parameter containing:\n",
      "tensor([[ 0.3743,  0.1923, -0.0274,  0.2341],\n",
      "        [ 0.1714, -0.2908, -0.1384, -0.4539]], requires_grad=True)\n",
      "Plaintext Bias: Parameter containing:\n",
      "tensor([-0.4228, -0.4911], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Instantiate single Linear layer\n",
    "layer_enc = crypten.nn.Linear(4, 2)\n",
    "\n",
    "#The weights and the bias are initialized to small random values\n",
    "print(\"Plaintext Weights:\", layer_enc._parameters['weight'])\n",
    "print(\"Plaintext Bias:\", layer_enc._parameters['bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Encrypted: True\n",
      "Bias Encrypted: True\n",
      "\n",
      "Encrypted Weights:\n",
      " tensor([[ 24533,  12599,  -1797,  15344],\n",
      "        [ 11230, -19056,  -9073, -29748]])\n",
      "Encrypted Bias:\n",
      " tensor([-27706, -32185])\n"
     ]
    }
   ],
   "source": [
    "#Encrypt the linear layer \n",
    "#TODO: encrypt() needs a src!\n",
    "layer_enc.encrypt()\n",
    "\n",
    "#Let's examine the weights and the bias again\n",
    "#First, we'll see that weight and bias have become MPCTensors\n",
    "print(\"Weights Encrypted:\", crypten.is_encrypted_tensor(layer_enc._parameters['weight']))\n",
    "print(\"Bias Encrypted:\", crypten.is_encrypted_tensor(layer_enc._parameters['bias']))\n",
    "print()\n",
    "\n",
    "#Now let's look at the tensor values\n",
    "print(\"Encrypted Weights:\\n\", layer_enc._parameters['weight']._tensor._tensor)\n",
    "print(\"Encrypted Bias:\\n\", layer_enc._parameters['bias']._tensor._tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypted data:\n",
      " tensor([[57011, 41316, 12289, 22436],\n",
      "        [10879, 18654,  3557, 59101],\n",
      "        [63182, 61965, 40347, 50172]])\n",
      "\n",
      "Encrypted result:\n",
      " tensor([[  6494, -46314],\n",
      "        [ -6308, -63064],\n",
      "        [ 18498, -67735]])\n",
      "\n",
      "Decrypted result:\n",
      " tensor([[ 0.0991, -0.7067],\n",
      "        [-0.0963, -0.9623],\n",
      "        [ 0.2823, -1.0336]])\n"
     ]
    }
   ],
   "source": [
    "#Now let's encrypt our data\n",
    "toy_data_enc = crypten.cryptensor(toy_data)\n",
    "print(\"Encrypted data:\\n\", toy_data_enc._tensor._tensor)\n",
    "print()\n",
    "\n",
    "#apply the encrypted layer: encrypted linear transformation \n",
    "result_enc = layer_enc.forward(toy_data_enc)\n",
    "print(\"Encrypted result:\\n\", result_enc._tensor._tensor)\n",
    "print()\n",
    "\n",
    "#decrypt the result:\n",
    "print(\"Decrypted result:\\n\", result_enc.get_plain_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the application of the encrypted linear layer on the encrypted data produces an encrypted result, which we can then decrypt to get the values in plaintext.\n",
    "\n",
    "Let's look at a second linear transformation, to give a flavor of how accuracy is preserved even when the data and the layer are encrypted. We'll look at a uniform scaling transformation, in which all tensor elements are multiplied by the same scalar factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypted Weights:\n",
      " tensor([[327680,      0,      0],\n",
      "        [     0, 327680,      0],\n",
      "        [     0,      0, 327680]])\n",
      "Encrypted Bias:\n",
      " tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "#Initialize a linear layer with random weights\n",
    "layer_enc = crypten.nn.Linear(3, 3)\n",
    "\n",
    "#Construct a uniform scaling matrix: we'll scale by factor 5\n",
    "factor = 5\n",
    "layer_enc._parameters['weight'] = torch.eye(3)*factor\n",
    "layer_enc._parameters['bias'] = torch.zeros_like(layer_enc._parameters['bias'])\n",
    "\n",
    "#Encrypt the layer\n",
    "layer_enc.encrypt()\n",
    "print(\"Encrypted Weights:\\n\", layer_enc._parameters['weight']._tensor._tensor)\n",
    "print(\"Encrypted Bias:\\n\", layer_enc._parameters['bias']._tensor._tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypted data:\n",
      " tensor([[65536, 65536, 65536],\n",
      "        [65536, 65536, 65536]])\n",
      "Encrypted result:\n",
      " tensor([[327680, 327680, 327680],\n",
      "        [327680, 327680, 327680]])\n",
      "\n",
      "Plaintext result:\n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "#Construct some toy data\n",
    "toy_data = torch.ones(2, 3)\n",
    "\n",
    "#Encrypt the toy data\n",
    "toy_data_enc = crypten.cryptensor(toy_data)\n",
    "print(\"Encrypted data:\\n\", toy_data_enc._tensor._tensor)\n",
    "\n",
    "#Apply the encrypted scaling transformation\n",
    "result_enc = layer_enc.forward(toy_data_enc)\n",
    "print(\"Encrypted result:\\n\", result_enc._tensor._tensor)\n",
    "print()\n",
    "\n",
    "print(\"Plaintext result:\\n\", result_enc.get_plain_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plaintext tensor is correctly scaled, even though we applied the encrypted transformation on the encrypted input! \n",
    "\n",
    "### Multi-layer Neural Networks\n",
    "Let's now look at how the encrypted input moves through an encrypted multi-layer neural network. To keep the explanations simple, we'll use a network with two linear layers and ReLU activations. We'll assume Alice has a private network that Bob wants to run encrypted inference on.\n",
    "\n",
    "To simulate this, we'll once again generate some toy data and train Alice's network on it. Then we'll encrypt Alice's network, Bob's data, and step through every layer in the network with the encrypted data. Through this, we'll see how the computations get applied although the network and the data are encrypted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll generate some random data for illustration purposes.\n",
    "features = 50\n",
    "examples = 100\n",
    "data = torch.randn(examples, features)\n",
    "w_true = torch.randn(1, features)\n",
    "b_true = torch.randn(1)\n",
    "y = w_true.matmul(data.t()) + b_true\n",
    "y = y.sign()\n",
    "\n",
    "#change labels to format needed by training\n",
    "y2 = torch.where(y==-1, torch.zeros(y.size()), y)\n",
    "y2 = y2.squeeze().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Alice and Bob's data\n",
    "data_alice = data[:90,:]\n",
    "data_bob = data[90:,:]\n",
    "\n",
    "label_alice = y2[:90]\n",
    "label_bob = y2[90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Loss: 0.09137161076068878\n",
      "Epoch 199 Loss: 0.029487576335668564\n",
      "Epoch 299 Loss: 0.01661916822195053\n",
      "Epoch 399 Loss: 0.011747055687010288\n",
      "Epoch 499 Loss: 0.00933974701911211\n"
     ]
    }
   ],
   "source": [
    "#Alice creates a very simple one layer network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Define Alice's network\n",
    "class AliceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AliceNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(50, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = AliceNet()\n",
    "\n",
    "#Train Alice's network\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(500):  \n",
    "    #forward pass: compute prediction\n",
    "    output = model(data_alice)\n",
    "    \n",
    "    #compute and print loss\n",
    "    loss = criterion(output, label_alice)\n",
    "    if i % 100 == 99:\n",
    "        print(\"Epoch\", i, \"Loss:\", loss.item())\n",
    "    \n",
    "    #zero gradients for learnable parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #backward pass: compute gradient with respect to model parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    #update model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 5 \tModule: <crypten.nn.module.Linear object at 0x7f9960458278>\n",
      "Name: 6 \tModule: <crypten.nn.module.ReLU object at 0x7f99604582e8>\n",
      "Name: output \tModule: <crypten.nn.module.Linear object at 0x7f99604582b0>\n"
     ]
    }
   ],
   "source": [
    "#Alice encrypts her network\n",
    "\n",
    "#Create dummy input of the correct input shape for the model\n",
    "dummy_input = torch.empty((1, 50))\n",
    "\n",
    "#Encrypt the network\n",
    "alice_private_model = crypten.nn.from_pytorch(model, dummy_input)\n",
    "alice_private_model.encrypt()\n",
    "\n",
    "#Let's look at what the encrypted network looks like\n",
    "for name, curr_module in alice_private_model._modules.items():\n",
    "    print(\"Name:\", name, \"\\tModule:\", curr_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the encrypted networks has 3 modules, named '5', '6' and 'output', denoting the first Linear layer, the ReLU activation, and the second Linear layer respectively. These modules are encrypted just as the layers in the previous section were. \n",
    "\n",
    "Now let's encrypt Bob's data, and step it through each encrypted module. For readability, we will use only 3 examples from Bob's data to illustrate the inference. Note how Bob's data remains encrypted after each individual layer's computation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: src=1\n",
    "data_bob_enc = crypten.cryptensor(data_bob[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Linear Layer:\n",
      " Output Encrypted: True\n",
      " Tensor values:\n",
      " tensor([[ -99751,   20290,  -12783,   30416,   -2868,   41344,  -28589,   15760,\n",
      "          -31889,   22338,   67787,   23732,   68546,   70207,  -45290,   42330,\n",
      "           49006,  -56520,   23644,  -25871],\n",
      "        [ -66496,    6791,    9580,   35294,  139194,  -51500,   19774,  -12889,\n",
      "           17738,    4093,   69535, -119679,   32774,   25008,  -13472,   16878,\n",
      "          119987,   88731,   83779,  -61285],\n",
      "        [  81934,   25924,  107973,   29638,   24051,  -25586,   31877,   44093,\n",
      "           22983,  -46466,   -1503,   34794,     953,  -53280,   39561,  -20057,\n",
      "           16802,   -8360,   23715,   76248]])\n"
     ]
    }
   ],
   "source": [
    "#forward through the first layer\n",
    "out_enc = alice_private_model._modules['5'].forward(data_bob_enc)\n",
    "print(\"First Linear Layer:\\n Output Encrypted:\", crypten.is_encrypted_tensor(out_enc))\n",
    "print(\" Tensor values:\\n\", out_enc._tensor._tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU:\n",
      " Output type: True\n",
      " Tensor values:\n",
      " tensor([[     0,  20290,      0,  30416,      0,  41344,      0,  15760,      0,\n",
      "          22338,  67787,  23732,  68546,  70207,      0,  42330,  49006,      0,\n",
      "          23644,      0],\n",
      "        [     0,   6791,   9580,  35294, 139194,      0,  19774,      0,  17738,\n",
      "           4093,  69535,      0,  32774,  25008,      0,  16878, 119987,  88731,\n",
      "          83779,      0],\n",
      "        [ 81934,  25924, 107973,  29638,  24051,      0,  31877,  44093,  22983,\n",
      "              0,      0,  34794,    953,      0,  39561,      0,  16802,      0,\n",
      "          23715,  76248]])\n"
     ]
    }
   ],
   "source": [
    "#apply ReLU activation\n",
    "out_enc = alice_private_model._modules['6'].forward(out_enc)\n",
    "print(\"ReLU:\\n Output type:\", crypten.is_encrypted_tensor(out_enc))\n",
    "print(\" Tensor values:\\n\", out_enc._tensor._tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Linear layer:\n",
      " Output Encrypted: True\n",
      " Tensor values:\n",
      " tensor([[ -48232,  128517],\n",
      "        [ 206110, -142978],\n",
      "        [  -7867,   63898]])\n"
     ]
    }
   ],
   "source": [
    "#forward through the second Linear layer\n",
    "out_enc = alice_private_model._modules['output'].forward(out_enc)\n",
    "print(\"Second Linear layer:\\n Output Encrypted:\", crypten.is_encrypted_tensor(out_enc)), \n",
    "print(\" Tensor values:\\n\", out_enc._tensor._tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrypted output:\n",
      " Output Encrypted: False\n",
      " Tensor values:\n",
      " tensor([[-0.7360,  1.9610],\n",
      "        [ 3.1450, -2.1817],\n",
      "        [-0.1200,  0.9750]])\n"
     ]
    }
   ],
   "source": [
    "#decrypt the output\n",
    "out_dec = out_enc.get_plain_text()\n",
    "print(\"Decrypted output:\\n Output Encrypted:\", crypten.is_encrypted_tensor(out_dec))\n",
    "print(\" Tensor values:\\n\", out_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we emphasize that the output of each layer is an encrypted tensor. Only after the final call to ```get_plain_text``` do we get the plaintext tensor.\n",
    "\n",
    "We have used a simple two-layer network in our example, but the same ideas apply to more complex networks and operations, and this is what allows CrypTen to successfully perform inference while keeping every computation encrypted. For more details, please refer to the CrypTen whitepaper."
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "511641209674539"
  },
  "disseminate_notebook_info": {
   "bento_version": "20190826-030256",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/shobha/fbsource/fbcode/bento/kernels/local/cryptenk/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "139530",
   "others_can_edit": true,
   "reviewers": "",
   "revision_id": "417315762230172",
   "tags": "",
   "tasks": "",
   "title": "Tutorial 5 -- Under the hood of Encrypted Networks"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
