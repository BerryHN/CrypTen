{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this tutorial, we'll look at how we can achieve the <i>Model Hiding</i> application we discussed in the Introduction. That is, let's say Alice has a trained model she wishes to keep private, and Bob has some data he wishes to classify while keeping it private. We'll see how CrypTen allows Alice and Bob coordinate and classify the data, all while achieving their privacy requirements.\n",
    "\n",
    "To simulate this scenario, we'll begin by Alice training a simple neural network on MNIST data. Then we'll see how Alice and Bob encrypt their network and data respectively, classify the encrypted data and finally decrypt the labels.\n",
    "\n",
    "### Initialization\n",
    "Let's load some MNIST data, and train Alice's network on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import torch\n",
    "\n",
    "crypten.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "#download data\n",
    "mnist_train = datasets.MNIST(\"/tmp\", download=True, train=True)\n",
    "mnist_test = datasets.MNIST(\"/tmp\", download=True, train=False)\n",
    "\n",
    "#compute normalization factors\n",
    "data_all = torch.cat([mnist_train.data, mnist_test.data]).float()\n",
    "data_mean, data_std = data_all.mean(), data_all.std()\n",
    "tensor_mean, tensor_std = data_mean.unsqueeze(0), data_std.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define Alice's and Bob's data\n",
    "data_alice = mnist_train.data\n",
    "data_bob = mnist_test.data\n",
    "\n",
    "label_alice = mnist_train.targets\n",
    "label_bob = mnist_test.targets\n",
    "\n",
    "#Normalize the data\n",
    "data_alice_norm = transforms.functional.normalize(data_alice.float(), tensor_mean, tensor_std)\n",
    "data_bob_norm = transforms.functional.normalize(data_bob.float(), tensor_mean, tensor_std)\n",
    "\n",
    "#Flatten the data\n",
    "data_alice_flat = data_alice_norm.flatten(start_dim=1)\n",
    "data_bob_flat = data_bob_norm.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.32299792766571045\n",
      "\tAccuracy: tensor([94.7917])\n",
      "Epoch 1 Loss: 0.2552248537540436\n",
      "\tAccuracy: tensor([97.9167])\n",
      "Epoch 2 Loss: 0.21967248618602753\n",
      "\tAccuracy: tensor([97.9167])\n",
      "Epoch 3 Loss: 0.19773970544338226\n",
      "\tAccuracy: tensor([97.9167])\n",
      "Epoch 4 Loss: 0.1811065673828125\n",
      "\tAccuracy: tensor([98.9583])\n"
     ]
    }
   ],
   "source": [
    "#Alice creates and trains her network on her data\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Define Alice's network\n",
    "class AliceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AliceNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        #self.batchnorm1 = nn.BatchNorm1d(128)\n",
    "        #self.batchnorm2 = nn.BatchNorm1d(128)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        #out = self.batchnorm1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        #out = self.batchnorm2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "model = AliceNet()\n",
    "\n",
    "#Train Alice's network\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-6)\n",
    "\n",
    "num_examples = 60000\n",
    "batch_size = 256\n",
    "num_epochs = 5\n",
    "log_accuracy = True\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for j in range(0, num_examples, batch_size):\n",
    "        \n",
    "        #get the mini-batch\n",
    "        start, end = j, min(j+batch_size,num_examples)\n",
    "        sample_flat = data_alice_flat[start:end,:]\n",
    "        target = label_alice[start:end]\n",
    "        \n",
    "        #forward pass: compute prediction\n",
    "        output = model(sample_flat)\n",
    "\n",
    "        #compute and print loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        #zero gradients for learnable parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #backward pass: compute gradient with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        #update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    #log accuracy every epoch\n",
    "    if log_accuracy:\n",
    "        pred = output.argmax(1)\n",
    "        correct = pred.eq(target)\n",
    "        correct_count = correct.sum(0, keepdim=True).float()\n",
    "        accuracy = correct_count.mul_(100.0 / output.size(0))\n",
    "        print(\"Epoch\", i, \"Loss:\", loss.item())\n",
    "        print(\"\\tAccuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encryption\n",
    "Alice now has a trained neural network that can classify data. Let's see how we can use CrypTen to encrypt this network, so it can be used to classify data without revealing its parameters. \n",
    "\n",
    "In CrypTen, encrypting PyTorch network is straightforward: first, we call the function `from_pytorch` that sets up a CrypTen network from the PyTorch network. Then, we call `encrypt` on the CrypTen network to encrypt its parameters. After encryption, the CrypTen network can also decrypted (see the `decrypt` function).\n",
    "\n",
    "In addition to the PyTorch network, the `from_pytorch` function also requires a dummy input of the shape of the model's input -- this is a similar requirement to what we saw in Tutorial 3. (In the next tutorial, we'll take a closer look at Alice's CrypTen network, to understand the details of how the parameters of each layer are encrypted.)\n",
    "We'll also encrypt Bob's data -- again, this step is identical to what we've seen in Tutorial 3. We'll walk through an example below. \n",
    "\n",
    "<i><small>(Technical note: Since Jupyter notebooks only run a single process, we use a custom decorator `mpc.run_multiprocess` to simulate a multi-party world below.)</small><i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encryption flag in CrypTen model: True\n",
      "Encryption of Data: True\n"
     ]
    }
   ],
   "source": [
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def encrypt_model_and_data():\n",
    "    \n",
    "    rank = comm.get().get_rank()\n",
    "    \n",
    "    if rank == 0:\n",
    "        #Alice gets the trained model\n",
    "        plaintext_model = model\n",
    "    else:\n",
    "        #Bob gets a dummy model with random parameters\n",
    "        plaintext_model = AliceNet()\n",
    "        \n",
    "    #Encrypt the model    \n",
    "    #Create a dummy input with the same shape as the model input\n",
    "    dummy_input = torch.empty((1, 784))\n",
    "    #Construct a CrypTen network with the trained model and dummy_input\n",
    "    private_model = crypten.nn.from_pytorch(plaintext_model, dummy_input)\n",
    "    #Encrypt the CrypTen network. Alice has the real model, so we encrypt with src=0\n",
    "    private_model.encrypt(src=0)\n",
    "    #The model is now encrypted: we can check the model's 'encrypted' flag!\n",
    "    if rank == 0:\n",
    "        print(\"Encryption flag in CrypTen model:\", private_model.encrypted)\n",
    "    \n",
    "    if rank == 1:\n",
    "        #Bob gets the real data\n",
    "        plaintext_data = data_bob_flat\n",
    "    else:\n",
    "        #Alice gets an empty torch tensor of the same shape\n",
    "        plaintext_data = torch.empty(data_bob_flat.size())\n",
    "        \n",
    "    #Encrypt the data\n",
    "    #Bob has the real data, so we encrypt with src=1\n",
    "    data_enc = crypten.cryptensor(plaintext_data, src=1)\n",
    "    if rank == 0:\n",
    "        print(\"Encryption of Data:\", crypten.is_encrypted_tensor(data_enc))\n",
    "    #print(rank, data_enc._tensor, \"\\n\")\n",
    "        \n",
    "z = encrypt_model_and_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Encrypted Data with Encrypted Model\n",
    "We can finally use Alice's encrypted network to classify Bob's encrypted data. This step is identical to PyTorch, except we'll use the encrypted network and data instead of the plaintext versions that PyTorch uses. \n",
    "\n",
    "<small><i>(Technical note: We have simulated a multi-party world with the `@mpc.run_multiprocess` decorator in the previous cell. However, as a result, the variables loaded do not carry over from cell to cell as is customary in a notebook. Therefore, in order to illustrate next steps, we reinitialize  `model_enc` and `data_enc` both with `src=0`)</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is required for demonstrating the learning algorithm in our notebook. \n",
    "# As Jupyter notebooks run only a single process, the model and the data both need to be encrypted\n",
    "# with src=0 in order for the remaining code to run. In a regular CrypTen implementation \n",
    "# (see the CrypTen examples folder), data_enc would be encrypted with src=1 as shown in the cell above.\n",
    "dummy_input = torch.empty((1, 784))\n",
    "private_model = crypten.nn.from_pytorch(model, dummy_input)\n",
    "private_model.encrypt(src=0)\n",
    "data_enc = crypten.cryptensor(data_bob_flat, src=0) #This would use src=1 outside Jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'crypten.autograd_cryptensor.AutogradCrypTensor'>\n",
      "<class 'crypten.autograd_cryptensor.AutogradCrypTensor'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'crypten.mpc.mpc.MPCTensor'>\n",
      "<class 'crypten.autograd_cryptensor.AutogradCrypTensor'>\n",
      "<class 'crypten.autograd_cryptensor.AutogradCrypTensor'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'crypten.mpc.mpc.MPCTensor'>\n",
      "<class 'crypten.autograd_cryptensor.AutogradCrypTensor'>\n",
      "<class 'crypten.autograd_cryptensor.AutogradCrypTensor'>\n"
     ]
    }
   ],
   "source": [
    "#We run inference on the encrypted network with the encrypted data\n",
    "private_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "     output_enc = private_model(data_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this classification is encrypted. To see this, here let's just check whether the result is an encrypted tensor; in the next tutorial, we'll look into the values of tensor and confirm the encryption. \n",
    "\n",
    "We can now decrypt the result. As we discussed before, Alice and Bob both have access to the decrypted output of the model, and can both use this to obtain the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor encrypted: True\n",
      "Decrypted output:\n",
      " tensor([[-0.7364, -4.2769,  3.2639,  ..., 10.7419,  0.7018,  1.0107],\n",
      "        [ 0.5365,  1.4743,  9.1954,  ..., -3.7552,  1.2168, -7.6075],\n",
      "        [-4.3711,  6.7733,  1.0815,  ...,  1.2077, -0.3217, -2.0999],\n",
      "        ...,\n",
      "        [-5.5424, -6.8985, -3.4222,  ...,  1.9314,  4.1241,  4.9593],\n",
      "        [ 1.6874, -1.1822, -3.0257,  ..., -4.0624,  3.7262, -4.6972],\n",
      "        [ 2.2276, -5.1252,  1.8377,  ..., -5.8095, -1.3235, -5.8347]])\n",
      "Decrypted labels:\n",
      " tensor([7, 2, 1,  ..., 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "#The results are encrypted: \n",
    "print(\"Output tensor encrypted:\", crypten.is_encrypted_tensor(output_enc)) \n",
    "\n",
    "#Decrypting the result\n",
    "output = output_enc.get_plain_text()\n",
    "print(\"Decrypted output:\\n\", output)\n",
    "\n",
    "#Obtaining the labels\n",
    "pred = output.argmax(dim=1)\n",
    "print(\"Decrypted labels:\\n\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: tensor([96.7300])\n"
     ]
    }
   ],
   "source": [
    "#Finally, we'll compute the accuracy of the output:\n",
    "output = output_enc.get_plain_text()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = output.argmax(1)\n",
    "    correct = pred.eq(label_bob)\n",
    "    correct_count = correct.sum(0, keepdim=True).float()\n",
    "    accuracy = correct_count.mul_(100.0 / output.size(0))\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This completes our tutorial. While we have used a simple network here to illustrate the concepts, CrypTen provides primitives to allow for encryption of substantially more complex networks. In our examples section, we demonstrate how CrypTen can be used to encrypt LeNet and ResNet, among others."
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "390894444956881"
  },
  "disseminate_notebook_info": {
   "bento_version": "20190826-030256",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/shobha/fbsource/fbcode/bento/kernels/local/cryptenk/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "139932",
   "others_can_edit": true,
   "reviewers": "",
   "revision_id": "375902760006757",
   "tags": "",
   "tasks": "",
   "title": "Tutorial 4 -- Classification with Encrypted Neural Networks"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
