{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this tutorial, we'll look at how we can achieve the <i>Model Hiding</i> application we discussed in the Introduction. That is, let's say Alice has a trained model she wishes to keep private, and Bob has some data he wishes to classify while keeping it private. We'll see how CrypTen allows Alice and Bob coordinate and classify the data, all while achieving their privacy requirements.\n",
    "\n",
    "To simulate this scenario, we'll begin by Alice training a simple neural network on MNIST data. Then we'll see how Alice and Bob encrypt their network and data respectively, classify the encrypted data and finally decrypt the labels.\n",
    "\n",
    "### Initialization\n",
    "Let's load some MNIST data, and train Alice's network on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:DistributedCommunicator with rank 0\n",
      "INFO:root:==================\n",
      "INFO:root:World size = 1\n"
     ]
    }
   ],
   "source": [
    "import crypten\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "#download data\n",
    "mnist_train = datasets.MNIST(\"/tmp\", download=True, train=True)\n",
    "mnist_test = datasets.MNIST(\"/tmp\", download=True, train=False)\n",
    "\n",
    "#compute normalization factors\n",
    "data_all = torch.cat([mnist_train.data, mnist_test.data]).float()\n",
    "data_mean, data_std = data_all.mean(), data_all.std()\n",
    "tensor_mean, tensor_std = data_mean.unsqueeze(0), data_std.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define Alice's and Bob's data\n",
    "data_alice = mnist_train.data\n",
    "data_bob = mnist_test.data\n",
    "\n",
    "label_alice = mnist_train.targets\n",
    "label_bob = mnist_test.targets\n",
    "\n",
    "#Normalize the data\n",
    "data_alice_norm = transforms.functional.normalize(data_alice.float(), tensor_mean, tensor_std)\n",
    "data_bob_norm = transforms.functional.normalize(data_bob.float(), tensor_mean, tensor_std)\n",
    "\n",
    "#Flatten the data\n",
    "data_alice_flat = data_alice_norm.flatten(start_dim=1)\n",
    "data_bob_flat = data_bob_norm.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.16712146997451782\n",
      "\tAccuracy: tensor([96.8750])\n",
      "Epoch 1 Loss: 0.13901349902153015\n",
      "\tAccuracy: tensor([97.9167])\n",
      "Epoch 2 Loss: 0.1161004975438118\n",
      "\tAccuracy: tensor([97.9167])\n",
      "Epoch 3 Loss: 0.09419527649879456\n",
      "\tAccuracy: tensor([98.9583])\n",
      "Epoch 4 Loss: 0.08182505518198013\n",
      "\tAccuracy: tensor([98.9583])\n",
      "Epoch 5 Loss: 0.06977503001689911\n",
      "\tAccuracy: tensor([98.9583])\n",
      "Epoch 6 Loss: 0.05783069133758545\n",
      "\tAccuracy: tensor([98.9583])\n",
      "Epoch 7 Loss: 0.043654829263687134\n",
      "\tAccuracy: tensor([98.9583])\n",
      "Epoch 8 Loss: 0.027489224448800087\n",
      "\tAccuracy: tensor([98.9583])\n",
      "Epoch 9 Loss: 0.016932297497987747\n",
      "\tAccuracy: tensor([98.9583])\n",
      "Epoch 10 Loss: 0.010210790671408176\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 11 Loss: 0.006478141527622938\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 12 Loss: 0.0046099042519927025\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 13 Loss: 0.0036421415861696005\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 14 Loss: 0.002934912219643593\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 15 Loss: 0.002525407588109374\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 16 Loss: 0.002182767493650317\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 17 Loss: 0.0018071290105581284\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 18 Loss: 0.0015945704653859138\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 19 Loss: 0.0014006233541294932\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 20 Loss: 0.0012343848356977105\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 21 Loss: 0.0011115361703559756\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 22 Loss: 0.0010093527380377054\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 23 Loss: 0.0009112947736866772\n",
      "\tAccuracy: tensor([100.])\n",
      "Epoch 24 Loss: 0.0008430083398707211\n",
      "\tAccuracy: tensor([100.])\n"
     ]
    }
   ],
   "source": [
    "#Alice creates and trains her network on her data\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Define Alice's network\n",
    "class AliceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AliceNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "model = AliceNet()\n",
    "\n",
    "#Train Alice's network\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-6)\n",
    "\n",
    "num_examples = 60000\n",
    "batch_size = 256\n",
    "num_epochs = 25\n",
    "log_accuracy = True\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for j in range(0, num_examples, batch_size):\n",
    "        \n",
    "        #get the mini-batch\n",
    "        start, end = j, min(j+batch_size,num_examples)\n",
    "        sample_flat = data_alice_flat[start:end,:]\n",
    "        target = label_alice[start:end]\n",
    "        \n",
    "        #forward pass: compute prediction\n",
    "        output = model(sample_flat)\n",
    "\n",
    "        #compute and print loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        #zero gradients for learnable parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #backward pass: compute gradient with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        #update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    #log accuracy every epoch\n",
    "    if log_accuracy:\n",
    "        pred = output.argmax(1)\n",
    "        correct = pred.eq(target)\n",
    "        correct_count = correct.sum(0, keepdim=True).float()\n",
    "        accuracy = correct_count.mul_(100.0 / output.size(0))\n",
    "        print(\"Epoch\", i, \"Loss:\", loss.item())\n",
    "        print(\"\\tAccuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encryption\n",
    "Alice now has a trained neural network that can classify data. Let's see how we can use CrypTen to encrypt this network, so it can be used to classify data without revealing its parameters. \n",
    "\n",
    "In CrypTen, encrypting PyTorch network is straightforward: first, we call the function ```from_pytorch``` that sets up a CrypTen network from the PyTorch network. Then, we call ```encrypt``` on the CrypTen network to encrypt its parameters. After encryption, the CrypTen network can also decrypted (see the ```decrypt``` function). <b>TODO: encrypt needs a src, and Bob needs a dummy model.</b>\n",
    "\n",
    "In addition to the PyTorch network, the ```from_pytorch``` function also requires a dummy input of the shape of the model's input (this is a similar requirement to what we saw in Tutorial 3). \n",
    "\n",
    "We'll walk through an example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encryption flag in Alice's CrypTen model: True\n"
     ]
    }
   ],
   "source": [
    "#Alice encrypts her network\n",
    "\n",
    "#Create a dummy input with the same shape as the model input\n",
    "dummy_input = torch.empty((1, 784))\n",
    "\n",
    "#Construct a CrypTen network with the trained model and dummy_input\n",
    "alice_private_model = crypten.nn.from_pytorch(model, dummy_input)\n",
    "\n",
    "#Encrypt the CrypTen network\n",
    "alice_private_model.encrypt()\n",
    "\n",
    "#Alice's model is now encrypted: we can check the model's 'encrypted' flag!\n",
    "print(\"Encryption flag in Alice's CrypTen model:\", alice_private_model.encrypted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next tutorial, we'll take a closer look at Alice's CrypTen network, to understand the details of how the parameters of each layer are encrypted.\n",
    "\n",
    "Let's also encrypt Bob's data (this step is identical to what we've seen in Tutorial 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bob encrypts his data, note src=1 \n",
    "data_bob_enc = crypten.cryptensor(data_bob_flat, src=0) #TODO: src = 0 for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Encrypted Data with Encrypted Model\n",
    "We can finally use Alice's encrypted network to classify Bob's encrypted data. This step is identical to PyTorch, except we'll use the encrypted network and data instead of the plaintext versions that PyTorch uses. Thus, we can just do: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alice runs inference on her encrypted model with Bob's encrypted data\n",
    "alice_private_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "     output_enc = alice_private_model(data_bob_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this classification is encrypted. To see this, here let's just check whether the result is an encrypted tensor; in the next tutorial, we'll look into the values of tensor and confirm the encryption. \n",
    "\n",
    "Finally, we'll decrypt the result. As we discussed before, Alice and Bob both have access to the decrypted result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor encrypted: True\n",
      "\n",
      "Decrypted output:\n",
      " tensor([[-1.7982, -1.4551,  0.0343,  ..., 11.4704, -2.6817, -2.0960],\n",
      "        [-0.3612,  1.1524, 14.0475,  ..., -3.0585, -1.7713, -6.4185],\n",
      "        [-1.4942, 11.0817,  0.4863,  ..., -0.2121,  0.3387, -2.5970],\n",
      "        ...,\n",
      "        [-3.4620, -2.6589, -2.7017,  ...,  0.1182,  0.3336, -2.0951],\n",
      "        [ 0.5075, -2.0209, -6.8010,  ..., -3.7700,  3.5046, -5.3319],\n",
      "        [-0.1711, -2.8731, -2.3386,  ..., -2.8373, -2.0506, -3.5235]])\n"
     ]
    }
   ],
   "source": [
    "#The results are encrypted: \n",
    "print(\"Output tensor encrypted:\", crypten.is_encrypted_tensor(output_enc)) \n",
    "print()\n",
    "\n",
    "#Decrypting the result\n",
    "print(\"Decrypted output:\\n\", output_enc.get_plain_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: tensor([98.1100])\n"
     ]
    }
   ],
   "source": [
    "#Finally, we'll compute the accuracy of the output:\n",
    "output = output_enc.get_plain_text()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = output.argmax(1)\n",
    "    correct = pred.eq(label_bob)\n",
    "    correct_count = correct.sum(0, keepdim=True).float()\n",
    "    accuracy = correct_count.mul_(100.0 / output.size(0))\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This completes our tutorial. While we have used a simple network here to illustrate the concepts, CrypTen provides primitives to allow for encryption of substantially more complex networks. In our examples section, we demonstrate how CrypTen can be used to encrypt LeNet and ResNet, among others."
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "390894444956881"
  },
  "disseminate_notebook_info": {
   "bento_version": "20190826-030256",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/shobha/fbsource/fbcode/bento/kernels/local/cryptenk/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "139932",
   "others_can_edit": true,
   "reviewers": "",
   "revision_id": "375902760006757",
   "tags": "",
   "tasks": "",
   "title": "Tutorial 4 -- Classification with Encrypted Neural Networks"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
