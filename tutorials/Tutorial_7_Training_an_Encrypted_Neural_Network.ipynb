{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Encrypted Neural Network\n",
    "\n",
    "In this tutorial, we will walk through an example of how we can train a neural network with CrypTen. Like in Tutorial 3, this is particularly relevant for the <i>Feature Aggregation</i> and <i>Data Augmentation</i> scenarios. We will focus on the usual two-party setting and show how we can train an accurate neural network for digit classification on the MNIST data.\n",
    "\n",
    "The tutorial will step through the <i>Feature Aggregation</i> scenario: Alice and Bob each have part of the features of the data set, and wish to train a neural network on their combined data, while keeping their data private. \n",
    "\n",
    "## Initialization\n",
    "As usual, we'll begin by importing and initializing the `crypten` and `torch` libraries.  \n",
    "\n",
    "As in Tutorial 3, we'll use the MNIST data to demonstrate how Alice and Bob can learn without revealing protected information. For reference, the feature size of each example in the MNIST data is `28 x 28`, and there are 60000 examples in training data and 10000 examples in the test data. \n",
    "\n",
    "As in Tutorial 3, let's assume Alice has the first `28 x 20` features and Bob has last `28 x 8` features. One way to think of this split is that Alice has the (roughly) top 2/3rds of each image, while Bob has the bottom 1/3rd of each image. We'll again use our helper script `mnist_utils.py` that downloads the publicly available MNIST data, and splits the data as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "crypten.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../examples/mnist_utils.py --option features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will essentially follow the same steps as in Tutorial 3 to load and encrypt Alice's and Bob's data, and then combine their encrypted data (i.e., Steps (a) to (c)). Step (d) is different, as we are now training a neural network instead of a linear SVM. \n",
    "\n",
    "We'll define the network architecture below, and then describe how to train it on encrypted data in the next section. For simplicity, we will restrict our problem to binary classification: we'll simply learn how to distinguish between 0 and non-zero digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Define an example network\n",
    "class ExampleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExampleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 12 * 12, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted Training\n",
    "\n",
    "After all the material we've covered in earlier tutorials, we only need to know a few additional items to remember in order to able to train on encrypted data. We'll describe these items first, and then illustrate them with small examples below. After that, we'll demonstrate how encrypted training works end-to-end.\n",
    "<ul>\n",
    "<li>We need to transform the input data to `AutogradCrypTensors` from `CrypTensors` before calling the forward pass.  (`AutogradCrypTensors` allow the CrypTensors to store gradients and thus enable backpropagation.) As we show in the examples below, this is easily done by simply calling the `AutogradCrypTensor` constructor with the previously encrypted `CrypTensor`.</li> \n",
    "<li>CrypTen training requires all labels to use one-hot encoding. This means that when using standard datasets such as MNIST, we need to modify the labels to use one-hot encoding.</li>\n",
    "<li>CrypTen does not use the PyTorch optimizers. It instead directly implements stochastic gradient descent on encrypted data. As we show in the examples below, using SGD in CrypTen is very similar to using the PyTorch optimizers.</li> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Transforming input data into AutogradCrypTensors\n",
    "from crypten.autograd_cryptensor import AutogradCrypTensor\n",
    "\n",
    "# Load Alice's data \n",
    "data_alice = crypten.load('/tmp/alice_train.pth', src=0)\n",
    "# Create a CrypTensor\n",
    "data_alice_enc = crypten.cryptensor(data_alice, src=0)\n",
    "# Create an AutogradCrypTensor from the CrypTensor\n",
    "data_alice_enc_auto = AutogradCrypTensor(data_alice_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now set up the data for our small example below\n",
    "# For illustration purposes, we will create random toy data\n",
    "x_small = torch.rand(100, 1, 28, 28)\n",
    "y_small = torch.randint(1, (100,))\n",
    "\n",
    "# Transform labels into one-hot encoding\n",
    "label_eye = torch.eye(2)\n",
    "y_one_hot = label_eye[y_small]\n",
    "\n",
    "# Transform all data to AutogradCrypTensors\n",
    "x_train = AutogradCrypTensor(crypten.cryptensor(x_small, src=0))\n",
    "y_train = AutogradCrypTensor(crypten.cryptensor(y_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.6192\n",
      "Epoch: 1 Loss: 0.6033\n"
     ]
    }
   ],
   "source": [
    "# Example: Stochastic Gradient Descent in CrypTen\n",
    "model_plaintext = ExampleNet()\n",
    "\n",
    "# Encrypt the model: This step is identical to Tutorial 4\n",
    "dummy_input = torch.empty((1, 1, 28, 28))\n",
    "model = crypten.nn.from_pytorch(model_plaintext, dummy_input)\n",
    "model.train()\n",
    "model.encrypt()\n",
    "\n",
    "# Choose loss functions\n",
    "loss = crypten.nn.MSELoss()\n",
    "\n",
    "# Set parameters: learning rate, num_epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2\n",
    "\n",
    "# Train the model: SGD on encrypted data\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # forward pass\n",
    "    output = model(x_train)\n",
    "    loss_value = loss(output, y_train)\n",
    "    \n",
    "    # set gradients to zero\n",
    "    model.zero_grad()\n",
    "\n",
    "    # perform backward pass\n",
    "    loss_value.backward()\n",
    "\n",
    "    # update parameters\n",
    "    model.update_parameters(learning_rate) \n",
    "    \n",
    "    # examine the loss after each epoch\n",
    "    print(\"Epoch: {0:d} Loss: {1:.4f}\".format(i, loss_value.get_plain_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now put these pieces together for a complete example that trains a network from scratch in a multi-party setting. As in Tutorial 3, we'll assume Alice has the rank 0 process, and Bob has the rank 1 process; so we'll load and encrypt Alice's data with `src=0`, and load and encrypt Bob's data with `src=1`. We'll then initialize a plaintext model and convert it to an encrypted model, just as we did in Tutorial 4. We'll finally define our loss function, training parameters, and run SGD on the encrypted data. For the purposes of this tutorial we train on 100 samples; training should complete in ~3 minutes per epoch.\n",
    "\n",
    "<small><i>(Technical note: Since Jupyter notebooks only run a single process, we use a custom decorator mpc.run_multiprocess to simulate a multi-party world below.)</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in progress:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed. Loss: 0.1800 Accuracy: 80.0000 \n",
      "Epoch 0 completed. Loss: 0.1800 Accuracy: 80.0000 \n",
      "\n",
      "\n",
      "Epoch 1 in progress:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss: 0.1436 Accuracy: 80.0000 \n",
      "Epoch 1 completed. Loss: 0.1436 Accuracy: 80.0000 \n",
      "\n",
      "\n",
      "Epoch 2 in progress:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Loss: 0.1215 Accuracy: 80.0000 \n",
      "Epoch 2 completed. Loss: 0.1215 Accuracy: 80.0000 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "import sys\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def run_encrypted_training():\n",
    "    # Load data:\n",
    "    # Alice's data gets loaded with src=0\n",
    "    x_alice = crypten.load('/tmp/alice_train.pth', src=0)\n",
    "    # Bob's data gets loaded with src=1\n",
    "    x_bob = crypten.load('/tmp/bob_train.pth', src=1)\n",
    "\n",
    "    # Encrypt the data: \n",
    "    # Alice's tensor gets encrypted with src=0\n",
    "    x_alice_enc = crypten.cryptensor(x_alice, src=0)\n",
    "    # Bob's tensor gets encrypted with src=1\n",
    "    x_bob_enc = crypten.cryptensor(x_bob, src=1)\n",
    "    \n",
    "    # using crypten.cat to combine the feature sets: identical to Tutorial 3\n",
    "    x_combined_enc = crypten.cat([x_alice_enc, x_bob_enc], dim=2)\n",
    "    x_combined_enc = x_combined_enc.unsqueeze(1)\n",
    "    \n",
    "    # Restrict training to 100 examples for speed\n",
    "    x_small = x_combined_enc[:100]\n",
    "    \n",
    "    # Load labels and restrict to the 100 examples\n",
    "    y_all = crypten.load('/tmp/train_labels.pth')\n",
    "    y_small = (y_all[:100]).long()\n",
    "    # Modify the labels so that:\n",
    "    # all non-zero digits have class label 1.\n",
    "    # all zero digits have class label 0\n",
    "    y_small[y_small == 0] = 0\n",
    "    y_small[y_small != 0] = 1\n",
    "    \n",
    "    # Initialize a plaintext model and encrypt: identical to Tutorial 4\n",
    "    model_plaintext = ExampleNet()\n",
    "    dummy_input = torch.empty((1, 1, 28, 28))\n",
    "    model = crypten.nn.from_pytorch(model_plaintext, dummy_input)\n",
    "    model.train()\n",
    "    model.encrypt()\n",
    "    \n",
    "    # Define a loss function\n",
    "    loss = crypten.nn.MSELoss()\n",
    "\n",
    "    # Define training parameters\n",
    "    num_epochs = 3\n",
    "    learning_rate = 0.001\n",
    "    num_examples = x_small.size(0)\n",
    "    log_progress = 5\n",
    "    batch_size = log_progress \n",
    "    \n",
    "    rank = comm.get().get_rank()\n",
    "    for i in range(num_epochs):  \n",
    "        last_progress_logged = 0\n",
    "        # Print output only from rank 0 process for readability\n",
    "        if rank == 0:\n",
    "            print(f\"Epoch {i} in progress:\")\n",
    "        \n",
    "        pbar = tqdm(range(0, num_examples, batch_size), leave=False)\n",
    "        for j in pbar:\n",
    "            \n",
    "            # define the start and end of the training mini-batch\n",
    "            start, end = j, min(j + batch_size, num_examples)\n",
    "            \n",
    "            # construct AutogradCrypTensors out of training examples\n",
    "            x_train = AutogradCrypTensor(x_small[start:end])\n",
    "            y_one_hot = label_eye[y_small[start:end]]\n",
    "            y_train = AutogradCrypTensor(crypten.cryptensor(y_one_hot))\n",
    "            \n",
    "            # perform forward pass:\n",
    "            output = model(x_train)\n",
    "            loss_value = loss(output, y_train)\n",
    "            \n",
    "            # set gradients to \"zero\" \n",
    "            model.zero_grad()\n",
    "\n",
    "            # perform backward pass: \n",
    "            loss_value.backward()\n",
    "\n",
    "            # update parameters\n",
    "            model.update_parameters(learning_rate)  \n",
    "            \n",
    "            # log progress every x examples:\n",
    "            if j+batch_size - last_progress_logged >= log_progress:\n",
    "                last_progress_logged += log_progress\n",
    "                pbar.set_description(f\"Loss {loss_value.get_plain_text().item():.4f}\")\n",
    "                          \n",
    "        # compute accuracy every epoch\n",
    "        pred = output.get_plain_text().argmax(1)\n",
    "        correct = pred.eq(y_small[start:end])\n",
    "        correct_count = correct.sum(0, keepdim=True).float()\n",
    "        accuracy = correct_count.mul_(100.0 / output.size(0))\n",
    "        print(\"Epoch {0:d} completed. Loss: {1:.4f} Accuracy: {2:.4f} \\n\".format(i, \n",
    "                                                                                loss_value.get_plain_text().item(), \n",
    "                                                                                accuracy.item()))\n",
    "\n",
    "z = run_encrypted_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
