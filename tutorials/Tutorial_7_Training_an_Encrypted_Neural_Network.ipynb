{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk through an example of how we can train a neural network with CrypTen. Like in Tutorial 3, this is particularly relevant for the <i>Feature Aggregation</i> and <i>Data Augmentation</i> scenarios. We will focus on the usual two-party setting and show how we can train an accurate neural network for digit classification on the MNIST data.\n",
    "\n",
    "The tutorial will step through the <i>Feature Aggregation</i> scenario: Alice and Bob each have part of the features of the data set, and wish to train a neural network on their combined data, while keeping their data private. \n",
    "\n",
    "### Initialization\n",
    "As usual, we'll begin by importing and initializing the `crypten` and `torch` libraries.  We'll then load the MNIST data. We'll keep things simple and learn a binary classifier, so we'll set our goal to distinguish the \"0\" digit and non-zero digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "crypten.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: torch.Size([60000, 28, 28])\n",
      "Test set: torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "mnist_train = datasets.MNIST(\"/tmp\", download=True, train=True)\n",
    "mnist_test = datasets.MNIST(\"/tmp\", download=True, train=False)\n",
    "\n",
    "#Modify the labels so that:\n",
    "# all non-zero digits have class label 1.\n",
    "# all zero digits have class label 0\n",
    "mnist_train.targets[mnist_train.targets != 0] = 1\n",
    "mnist_test.targets[mnist_test.targets != 0] = 1\n",
    "mnist_train.targets[mnist_train.targets == 0] = 0\n",
    "mnist_test.targets[mnist_test.targets == 0] = 0\n",
    "\n",
    "#Let's look at how many examples and features we have:\n",
    "print('Training set:', mnist_train.data.size())\n",
    "print('Test set:', mnist_test.data.size())\n",
    "\n",
    "#Compute normalization factors\n",
    "data_all = torch.cat([mnist_train.data, mnist_test.data]).float()\n",
    "data_mean, data_std = data_all.mean(), data_all.std()\n",
    "tensor_mean, tensor_std = data_mean.unsqueeze(0), data_std.unsqueeze(0)\n",
    "\n",
    "#Normalize the data\n",
    "data_train_norm = transforms.functional.normalize(mnist_train.data.float(), tensor_mean, tensor_std)\n",
    "data_test_norm = transforms.functional.normalize(mnist_test.data.float(), tensor_mean, tensor_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the feature size of each example is `28 x 28`, and there are 60000 examples in training data and 10000 examples in the test data.\n",
    "\n",
    "Like we did in Tutorial 3, let's assume Alice has the first `28 x 20` features in a tensor called `data_alice` and Bob has last `28 x 8` features in a tensor called `data_bob`. One way to think of this split is that Alice has the (roughly) top 2/3rds of each image, while Bob has the bottom 1/3rd of each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of data_alice and data_bob. \n",
    "data_alice = data_train_norm[:,:,:20]\n",
    "data_bob = data_train_norm[:,:,20:]\n",
    "train_labels = mnist_train.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we assume that Alice has the `rank` 0 process and Bob has the `rank` 1 process. We follow the same steps as in Tutorial 3 to encrypt their data, and then combine their encrypted data (see Steps (a) to (c); omitted here for brevity). Step (d) is different, as we are now training a neural network instead of a linear SVM. We'll define the network architecture below, and then describe how to train it on encrypted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Define an example network\n",
    "class ExampleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExampleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 12 * 12, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encrypted Training\n",
    "\n",
    "After all the material we've covered in earlier tutorials, we only need to know a few additional items to remember in order to able to train on encrypted data. We'll describe these items first, and then illustrate them with small examples below. After that, we'll demonstrate how encrypted training works end-to-end.\n",
    "<ul>\n",
    "<li>We need to transform the input data to `AutogradCrypTensors` from `CrypTensors` before calling the forward pass.  (`AutogradCrypTensors` allow the CrypTensors to store gradients and thus enable backpropagation.) As we show in the examples below, this is easily done by simply calling the `AutogradCrypTensor` constructor with the previously encrypted `CrypTensor`.</li> \n",
    "<li>CrypTen training requires all labels to use one-hot encoding. This means that when using standard datasets such as MNIST, we need to modify the labels to use one-hot encoding.</li>\n",
    "<li>CrypTen does not use the PyTorch optimizers. It instead directly implements stochastic gradient descent on encrypted data. As we show in the examples below, using SGD in CrypTen is very similar to using the PyTorch optimizers.</li> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Transforming input data into AutogradCrypTensors\n",
    "from crypten.autograd_cryptensor import AutogradCrypTensor\n",
    "\n",
    "# Create a CrypTensor\n",
    "data_alice_enc = crypten.cryptensor(data_alice, src=0)\n",
    "# Create an AutogradCrypTensor from the CrypTensor\n",
    "data_alice_enc_auto = AutogradCrypTensor(data_alice_enc)\n",
    "\n",
    "# We'll now set up the data for our small example below\n",
    "# For illustration purposes, we'll use the complete\n",
    "# normalized data (all features) but use only 100 examples \n",
    "x_reduced = data_train_norm[:100].unsqueeze(1)\n",
    "y_reduced = train_labels[:100]\n",
    "\n",
    "# Transform labels into one-hot encoding\n",
    "label_eye = torch.eye(2)\n",
    "y_one_hot = label_eye[y_reduced]\n",
    "\n",
    "# Transform all data to AutogradCrypTensors\n",
    "x_train = AutogradCrypTensor(crypten.cryptensor(x_reduced, src=0))\n",
    "y_train = AutogradCrypTensor(crypten.cryptensor(y_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.4958\n",
      "Epoch: 1 Loss: 0.4707\n"
     ]
    }
   ],
   "source": [
    "# Example: Stochastic Gradient Descent in CrypTen\n",
    "model_plaintext = ExampleNet()\n",
    "\n",
    "# Encrypt the model: This step is identical to Tutorial 4\n",
    "dummy_input = torch.empty((1, 1, 28, 28))\n",
    "model = crypten.nn.from_pytorch(model_plaintext, dummy_input)\n",
    "model.train()\n",
    "model.encrypt()\n",
    "\n",
    "# Choose loss functions\n",
    "loss = crypten.nn.MSELoss()\n",
    "\n",
    "# Set parameters: learning rate, num_epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2\n",
    "\n",
    "# Train the model: SGD on encrypted data\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # forward pass\n",
    "    output = model(x_train)\n",
    "    loss_value = loss(output, y_train)\n",
    "    \n",
    "    # set gradients to zero\n",
    "    model.zero_grad()\n",
    "\n",
    "    # perform backward pass\n",
    "    loss_value.backward()\n",
    "\n",
    "    # update parameters\n",
    "    model.update_parameters(learning_rate) \n",
    "    \n",
    "    # examine the loss after each epoch\n",
    "    print(\"Epoch: {0:d} Loss: {1:.4f}\".format(i, loss_value.get_plain_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now put these pieces together for a complete example that trains a network from scratch in a multi-party setting. As in Tutorial 3, we'll encrypt `data_alice` with `src=0`, since Alice has the rank 0 process, and `data_bob` with `src=1`, since Bob has the rank 1 process. We'll then initialize a plaintext model and convert it to an encrypted model, just as we did in Tutorial 4. We'll finally define our loss function, training parameters, and run SGD on the encrypted data. \n",
    "\n",
    "<small><i>(Technical note: Since Jupyter notebooks only run a single process, we use a custom decorator mpc.run_multiprocess to simulate a multi-party world below.)</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial we train on 100 samples. Training should complete in ~3 minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in progress:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed. Loss: 0.2010 Accuracy: 80.0000 \n",
      "Epoch 0 completed. Loss: 0.2010 Accuracy: 80.0000 \n",
      "\n",
      "\n",
      "Epoch 1 in progress:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss: 0.1520 Accuracy: 80.0000 \n",
      "Epoch 1 completed. Loss: 0.1520 Accuracy: 80.0000 \n",
      "\n",
      "\n",
      "Epoch 2 in progress:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Loss: 0.1272 Accuracy: 80.0000 \n",
      "Epoch 2 completed. Loss: 0.1272 Accuracy: 80.0000 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "import sys\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def run_encrypted_training():\n",
    "    rank = comm.get().get_rank()\n",
    "\n",
    "    if rank == 0:\n",
    "        # load Alice's data\n",
    "        x_alice = data_alice\n",
    "    else:\n",
    "        # load dummy input with the same shape\n",
    "        x_alice = torch.empty(data_alice.size())\n",
    "\n",
    "    # Similarly, for Bob's data:\n",
    "    if rank == 1:\n",
    "        # load Bob's data\n",
    "        x_bob = data_bob\n",
    "    else:\n",
    "        # load dummy input\n",
    "        x_bob = torch.empty(data_bob.size())\n",
    "\n",
    "    # Encrypt the data: \n",
    "    # Alice's tensor gets encrypted with src=0\n",
    "    x_alice_enc = crypten.cryptensor(x_alice, src=0)\n",
    "    # Bob's tensor gets encrypted with src=1\n",
    "    x_bob_enc = crypten.cryptensor(x_bob, src=1)\n",
    "    \n",
    "    # using crypten.cat to combine the feature sets: identical to Tutorial 3\n",
    "    x_combined_enc = crypten.cat([x_alice_enc, x_bob_enc], dim=2)\n",
    "    x_combined_enc = x_combined_enc.unsqueeze(1)\n",
    "    \n",
    "    # Restrict training to 100 examples for speed\n",
    "    x_reduced = x_combined_enc[:100]\n",
    "    y_reduced = train_labels[:100]\n",
    "    \n",
    "    # Initialize a plaintext model and encrypt: identical to Tutorial 4\n",
    "    model_plaintext = ExampleNet()\n",
    "    dummy_input = torch.empty((1, 1, 28, 28))\n",
    "    model = crypten.nn.from_pytorch(model_plaintext, dummy_input)\n",
    "    model.train()\n",
    "    model.encrypt()\n",
    "    \n",
    "    # Define a loss function\n",
    "    loss = crypten.nn.MSELoss()\n",
    "\n",
    "    # Define training parameters\n",
    "    num_epochs = 3\n",
    "    learning_rate = 0.001\n",
    "    num_examples = x_reduced.size(0)\n",
    "    log_progress = 5\n",
    "    batch_size = log_progress \n",
    "    \n",
    "    for i in range(num_epochs):  \n",
    "        last_progress_logged = 0\n",
    "        # Print output only from rank 0 process for readability\n",
    "        if rank == 0:\n",
    "            print(f\"Epoch {i} in progress:\")\n",
    "        \n",
    "        pbar = tqdm(range(0, num_examples, batch_size), leave=False)\n",
    "        for j in pbar:\n",
    "            \n",
    "            # define the start and end of the training mini-batch\n",
    "            start, end = j, min(j + batch_size, num_examples)\n",
    "            \n",
    "            # construct AutogradCrypTensors out of training examples\n",
    "            x_train = AutogradCrypTensor(x_reduced[start:end])\n",
    "            y_one_hot = label_eye[y_reduced[start:end]]\n",
    "            y_train = AutogradCrypTensor(crypten.cryptensor(y_one_hot))\n",
    "            \n",
    "            # perform forward pass:\n",
    "            output = model(x_train)\n",
    "            loss_value = loss(output, y_train)\n",
    "            \n",
    "            # set gradients to \"zero\" \n",
    "            model.zero_grad()\n",
    "\n",
    "            # perform backward pass: \n",
    "            loss_value.backward()\n",
    "\n",
    "            # update parameters\n",
    "            model.update_parameters(learning_rate)  \n",
    "            \n",
    "            # log progress every x examples:\n",
    "            if j+batch_size - last_progress_logged >= log_progress:\n",
    "                last_progress_logged += log_progress\n",
    "                pbar.set_description(f\"Loss {loss_value.get_plain_text().item():.4f}\")\n",
    "                          \n",
    "        # compute accuracy every epoch\n",
    "        pred = output.get_plain_text().argmax(1)\n",
    "        correct = pred.eq(y_reduced[start:end])\n",
    "        correct_count = correct.sum(0, keepdim=True).float()\n",
    "        accuracy = correct_count.mul_(100.0 / output.size(0))\n",
    "        \n",
    "        print(\"Epoch {0:d} completed. Loss: {1:.4f} Accuracy: {2:.4f} \\n\".format(i, \n",
    "                                                                                loss_value.get_plain_text().item(), \n",
    "                                                                                accuracy.item()))\n",
    "\n",
    "z = run_encrypted_training()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
