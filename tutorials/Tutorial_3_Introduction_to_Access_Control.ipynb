{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can now start using CrypTen to carry out private computations in some common applications. In this tutorial, we will look at the first two applications described in the Introduction, <i>Feature Aggregation</i> and <i>Data Augmentation</i>. In both applications, we'll use a simple two-party setting and demonstrate how we can learn a linear SVM. In the process, we will see how access control works in CrypTen. We'll return to creating ```CrypTensors``` with the high-level ```crypten.cryptensor``` factory function, as we did in Tutorial 1.\n",
    "\n",
    "### Initialization\n",
    "As usual, we'll begin by importing the ```crypten``` and ```torch``` libraries. We'll load the MNIST data. Because we will be building a binary classifier, we'll set our goal to distinguish the \"0\" digit and non-zero digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:DistributedCommunicator with rank 0\n",
      "INFO:root:==================\n",
      "INFO:root:World size = 1\n"
     ]
    }
   ],
   "source": [
    "import crypten\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: torch.Size([60000, 28, 28])\n",
      "Test set: torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "mnist_train = datasets.MNIST(\"/tmp\", download=True, train=True)\n",
    "mnist_test = datasets.MNIST(\"/tmp\", download=True, train=False)\n",
    "\n",
    "#Modify the labels so that:\n",
    "# all non-zero digits have class label 1.\n",
    "# all zero digits have class label -1\n",
    "mnist_train.targets[mnist_train.targets != 0] = 1\n",
    "mnist_test.targets[mnist_test.targets != 0] = 1\n",
    "mnist_train.targets[mnist_train.targets == 0] = -1\n",
    "mnist_test.targets[mnist_test.targets == 0] = -1\n",
    "\n",
    "\n",
    "#Let's look at how many examples and features we have:\n",
    "print('Training set:', mnist_train.data.size())\n",
    "print('Test set:', mnist_test.data.size())\n",
    "\n",
    "#Compute normalization factors\n",
    "data_all = torch.cat([mnist_train.data, mnist_test.data]).float()\n",
    "data_mean, data_std = data_all.mean(), data_all.std()\n",
    "tensor_mean, tensor_std = data_mean.unsqueeze(0), data_std.unsqueeze(0)\n",
    "\n",
    "#Normalize the data\n",
    "data_train_norm = transforms.functional.normalize(mnist_train.data.float(), tensor_mean, tensor_std)\n",
    "data_test_norm = transforms.functional.normalize(mnist_test.data.float(), tensor_mean, tensor_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the feature size of each example is ```28 x 28```, and there are 60000 examples in training data and 10000 examples in the test data. \n",
    "\n",
    "### Application 1: Feature Aggregation\n",
    "In this application, two parties, Alice and Bob, each have a part of the features of the dataset. Let's assume Alice has the first ```28 x 20``` features in a tensor called ```data_alice``` and Bob has last ```28 x 8``` features in a tensor called ```data_bob```. One way to think of this split is that Alice has the (roughly) top 2/3rds of each image, while Bob has the bottom 1/3rd of each image. \n",
    "\n",
    "We'll see how we can use CrypTen to learn over all ```28 x 28``` features (i.e., the entire image), while keeping each party's features private."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of data_alice and data_bob. \n",
    "data_alice = data_train_norm[:,:,:20]\n",
    "data_bob = data_train_norm[:,:,20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrypTen runs a separate process for each party, but each process runs the identical (complete) program. We therefore need a mechanism to ensure that each process holds its data, and shares only the encrypted version with the other processes. \n",
    "\n",
    "As is standard in MPI programming, CrypTen uses a ```rank``` variable to identify the process (and thus the party). Let's assume Alice has the ```rank``` 0 process and Bob has the ```rank``` 1 process. We'll illustrate how Alice and Bob learn privately in 4 steps: (a) loading the data, (b) encrypting the data, (c) constructing the encrypted training data, and (d) training privately. \n",
    "\n",
    "#### Step (a): Loading the Data\n",
    "Our first step is to load each party's data into its process. To allow Alice to load her data, and ensure that Bob does not load her data, we do the following: \n",
    "<ol>\n",
    "<li> The running process checks its rank. </li>\n",
    "<li> If the rank is 0 (Alice's process), the process loads Alice's data. </li>\n",
    "<li> If the rank is not 0 (Bob's process), the process loads dummy input in the shape of Alice's data. </li>\n",
    "</ol>\n",
    "\n",
    "Bob's process needs to create the dummy input because it also needs to be aware of the size of Alice's data. (This is a requirement of ```torch.distributed```, our communication backend.) We follow similar steps to load Bob's data correctly with the ```rank``` 1 process. \n",
    "\n",
    "Let's now see what this looks like in CrypTen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crypten import comm\n",
    "\n",
    "#Find out which process is running\n",
    "rank = comm.get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    #load Alice's data\n",
    "    x_alice = data_alice\n",
    "else:\n",
    "    #load dummy input with the same shape\n",
    "    x_alice = torch.empty(data_alice.size())\n",
    "    \n",
    "    \n",
    "#Similarly, for Bob's data:\n",
    "if rank == 1:\n",
    "    #load Bob's data\n",
    "    x_bob = data_bob\n",
    "else:\n",
    "    #load dummy input\n",
    "    x_bob = torch.empty(data_bob.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step (b): Encrypting the data\n",
    "Next, we encrypt the data by creating ```CrypTensors```, just as we did in Tutorial 1.\n",
    "But here there is one crucial difference: we have to provide the ```CrypTensor``` with the source rank, i.e., rank of the process holding its (real) data. This is provided through the ```src``` keyword when creating the  ```CrypTensor```. \n",
    "\n",
    "In our example, when creating ```CrypTensor``` for Alice's data, we should use ```src=0```; when creating \n",
    "```CrypTensor``` for Bob's data, we should use ```src=1```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_alice_enc = crypten.cryptensor(x_alice, src=0)\n",
    "x_bob_enc = crypten.cryptensor(x_bob, src=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both the rank 0 and the rank 1 process construct both ```x_alice_enc``` and ```x_bob_enc``` tensors. However, the rank 0 process creates ```x_bob_enc``` based on dummy input, and only ```x_alice_enc``` based on the real data. The rank 1 process does the reverse: ```x_alice_enc``` based on dummy input and ```x_bob_enc``` based on real data. \n",
    "\n",
    "#### Step (c): Constructing the Encrypted Training Data\n",
    "To use both Alice's features and Bob's features for training, we'll construct a tensor that concatenates both encrypted tensors. We'll do this with CrypTen's ```cat``` function, similar to ```torch.cat```, and this creates a new ```CrypTensor```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Alice's encrypted data:  torch.Size([60000, 28, 20])\n",
      "Size of Bob's encrypted data:  torch.Size([60000, 28, 8])\n",
      "\n",
      "Size of the combined data:  torch.Size([60000, 28, 28])\n",
      "Combined data encrypted:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Alice's encrypted data: \", x_alice_enc.size()) \n",
    "print(\"Size of Bob's encrypted data: \", x_bob_enc.size())\n",
    "print()\n",
    "\n",
    "#using crypten.cat to combine the feature sets\n",
    "x_combined_enc = crypten.cat([x_alice_enc, x_bob_enc], dim=2)\n",
    "\n",
    "print(\"Size of the combined data: \", x_combined_enc.size())\n",
    "print(\"Combined data encrypted: \", crypten.is_encrypted_tensor(x_combined_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not reveal any private information by doing so: process 0 will construct a tensor that concatenates Alice's encrypted data and dummy input in the shape of ```x_bob```; process 1 will construct a tensor that concatenates Bob's encrypted data and dummy input in the shape of ```x_alice```.\n",
    "\n",
    "We can now use this data to train in CrypTen just as we would use plaintext data in PyTorch. \n",
    "\n",
    "#### Step (d): Training with Encrypted Data \n",
    "We'll now use a linear SVM classifier to show how CrypTen can train on encrypted data. CrypTen implements all of the necessary operations required for this (and many other) learning algorithms to operate on encrypted tensors, so we can implement the learning in the same way as we would on plaintext tensors. \n",
    "\n",
    "The code below implements the learning algorithm in CrypTen. While each step is carried out on ```CrypTensors```, the learning algorithm looks just as it would in PyTorch! The only difference is that, in CrypTen, the learned weights and bias are ```CrypTensors```. If the plaintext versions of weights and bias are required, Alice and Bob will have to agree to decrypt them at the end of the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use only the first 10k examples so it runs faster\n",
    "data_enc = x_combined_enc[:10000,:,:]\n",
    "labels = mnist_train.targets[:10000]\n",
    "examples = data_enc.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "--- Accuracy 79.39%\n",
      "Epoch 10\n",
      "--- Accuracy 81.97%\n",
      "Epoch 15\n",
      "--- Accuracy 84.27%\n",
      "Epoch 20\n",
      "--- Accuracy 86.33%\n",
      "Epoch 25\n",
      "--- Accuracy 87.93%\n",
      "Epoch 30\n",
      "--- Accuracy 89.03%\n",
      "Epoch 35\n",
      "--- Accuracy 90.01%\n",
      "Epoch 40\n",
      "--- Accuracy 90.89%\n",
      "Epoch 45\n",
      "--- Accuracy 91.52%\n",
      "Epoch 50\n",
      "--- Accuracy 92.06%\n"
     ]
    }
   ],
   "source": [
    "# Random initialization for linear svm\n",
    "w_init = torch.randn(1, 28*28)\n",
    "b_init = torch.randn(1)\n",
    " \n",
    "#We'll use only the first 10k examples so it runs faster\n",
    "x_combined_enc = x_combined_enc[:10000,:,:]\n",
    "labels = mnist_train.targets[:10000]\n",
    "\n",
    "# Turn all tensors into encrypted tensors\n",
    "y_enc = crypten.cryptensor(labels)   \n",
    "w_enc = crypten.cryptensor(w_init)\n",
    "b_enc = crypten.cryptensor(b_init)\n",
    "\n",
    "#define parameters: epoch and learning rate\n",
    "epochs = 50\n",
    "lr = 0.1\n",
    "log_accuracy = True\n",
    "\n",
    "x_flatten_enc = x_combined_enc.flatten(start_dim=1)\n",
    "\n",
    "for i in range(epochs):\n",
    "        # Forward\n",
    "        yhat = w_enc.matmul(x_flatten_enc.t()) + b_enc\n",
    "        yhat = yhat.sign()\n",
    "\n",
    "        yy = yhat * y_enc\n",
    "\n",
    "        if log_accuracy and i%5 == 4:\n",
    "            # Compute accuracy\n",
    "            correct = (yy + 1).mul(0.5).sum()\n",
    "            print(\"Epoch %d\" % (i + 1))\n",
    "            print(\n",
    "                \"--- Accuracy %.2f%%\"\n",
    "                % (correct.get_plain_text().float().div(examples).item() * 100)\n",
    "            )\n",
    "        # Backward\n",
    "        loss_grad = y_enc * (yy - 1) * 0.5\n",
    "\n",
    "        b_grad = loss_grad.sum()/examples\n",
    "        w_grad = loss_grad.matmul(x_flatten_enc)/examples\n",
    "\n",
    "        # Update\n",
    "        w_enc = w_enc - w_grad * lr\n",
    "        b_enc = b_enc - b_grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrypTen weights: tensor([[ 2.2888e-03, -1.2600e+00,  1.0137e+00,  9.7182e-01,  2.0956e+00,\n",
      "          4.3066e-01,  1.8904e+00,  5.4070e-01,  8.6337e-01,  9.3915e-01,\n",
      "          1.4293e+00,  1.4012e+00,  1.2209e+00,  1.0609e+00,  8.4262e-01,\n",
      "          9.7504e-03, -4.3462e-01,  2.2797e-02,  8.7093e-01, -6.3722e-01,\n",
      "         -7.9924e-01,  1.4096e+00, -1.3374e-01,  1.0419e-01,  3.4332e-02,\n",
      "         -2.0895e+00, -2.9851e-01,  6.4771e-01, -4.8135e-01,  5.9143e-01,\n",
      "          7.0868e-01,  6.7416e-01, -3.1268e-01,  7.9309e-01, -1.0678e+00,\n",
      "          9.0994e-01,  4.2400e-01, -9.4559e-01, -8.0109e-02,  1.1972e+00,\n",
      "         -1.3113e+00,  4.1290e-01, -1.0152e+00,  7.4556e-01,  6.6212e-01,\n",
      "          8.1783e-01,  1.0909e+00, -2.6259e-01,  8.9722e-01,  5.9947e-01,\n",
      "         -3.8681e-02,  2.3361e-02, -1.3056e+00, -7.6547e-01,  1.5474e+00,\n",
      "          5.2530e-01, -3.4320e-01,  1.9834e+00,  1.1899e-01, -2.9057e-01,\n",
      "         -2.3694e-01,  1.3698e+00, -1.6789e+00,  1.9891e+00, -6.3919e-01,\n",
      "          1.6766e+00, -1.4671e-01,  5.1503e-01, -6.9566e-01,  2.9094e-01,\n",
      "          1.3223e+00,  1.5208e-01, -1.1659e+00,  1.7816e+00,  1.1736e+00,\n",
      "         -1.0842e+00, -1.1336e+00, -1.5500e-01, -3.5371e-01, -1.3143e+00,\n",
      "         -1.2438e+00, -8.7938e-01, -1.2665e+00,  1.5445e-01, -1.8754e+00,\n",
      "          4.3686e-02, -2.4500e-01,  9.0540e-01, -5.7382e-01,  6.3541e-01,\n",
      "         -4.3756e-01,  8.2077e-02, -1.6982e+00,  4.6233e-01,  2.7713e-01,\n",
      "          1.4074e+00,  9.6077e-01,  1.0643e+00,  1.4301e+00,  1.0121e+00,\n",
      "          6.1414e-01, -1.1843e+00, -3.0424e-01, -2.6129e-01, -4.0605e-01,\n",
      "         -1.3436e+00,  3.3084e-01,  1.2944e+00,  9.8022e-01,  1.0744e+00,\n",
      "         -1.2145e+00,  2.4483e-01, -1.5996e+00,  1.0745e+00, -1.8788e+00,\n",
      "          2.3521e+00, -1.2276e-01,  7.6126e-01, -8.7097e-02, -5.9669e-01,\n",
      "          7.0036e-01,  3.0156e-01, -1.0344e+00,  1.3457e+00, -1.6929e+00,\n",
      "         -2.4979e-01,  5.7951e-01,  1.7929e-01,  1.6172e+00,  2.1794e+00,\n",
      "          1.7000e+00,  3.4622e-02,  6.5775e-01,  8.6266e-01, -9.8415e-01,\n",
      "          1.0391e+00,  9.6951e-01, -5.6058e-01, -1.1330e+00, -1.2198e+00,\n",
      "         -1.3396e-01,  6.3148e-01,  8.5696e-01, -4.4307e-01, -1.2562e+00,\n",
      "         -4.8141e-02, -2.6410e-01,  1.8939e+00, -2.1422e-01,  2.2205e-01,\n",
      "          1.6339e+00, -1.4135e+00, -4.2122e-01, -8.0325e-01,  9.5978e-01,\n",
      "         -7.7505e-01,  6.5744e-01, -2.5705e-01, -7.7957e-02, -2.7211e-01,\n",
      "          8.2835e-01, -1.0909e+00,  9.7517e-01, -1.0291e-01,  6.1195e-01,\n",
      "         -1.0750e-01,  1.0758e+00, -4.1124e-01, -6.3452e-01, -2.0909e+00,\n",
      "          3.9543e-01, -9.5969e-01, -3.0907e-01, -8.2523e-01, -6.8878e-01,\n",
      "         -8.3946e-01,  4.6544e-01, -2.4301e-01, -8.2840e-01, -6.1975e-01,\n",
      "          1.1383e-01,  9.0724e-01,  7.7380e-01, -1.1847e-01, -1.1875e+00,\n",
      "          1.1756e+00,  1.4157e+00, -1.2361e+00, -1.2641e+00,  1.1790e+00,\n",
      "         -1.7554e+00, -8.9189e-01,  1.1070e+00, -1.1350e+00,  9.1765e-01,\n",
      "         -1.0462e+00, -5.6110e-01,  1.8497e+00, -1.3053e+00,  7.5119e-01,\n",
      "         -1.7435e+00,  2.9506e-01,  4.2198e-01, -3.8567e-01,  3.7289e-01,\n",
      "         -4.4131e-01, -3.8229e-01, -4.1908e-01, -8.1650e-02, -1.0816e+00,\n",
      "         -7.7377e-01,  2.3189e-01,  4.0311e-01, -9.9060e-02,  2.9665e-01,\n",
      "         -4.2801e-01, -1.4911e+00, -1.1757e+00,  1.6212e+00,  6.2561e-02,\n",
      "         -4.7147e-01, -2.6096e+00, -1.0785e+00,  1.1740e-01, -9.9847e-01,\n",
      "          1.4157e+00, -3.0304e-02, -9.6397e-01, -1.1076e-01,  1.8188e-01,\n",
      "         -5.5908e-01,  8.6363e-01,  3.7964e-01,  1.2126e+00,  4.8094e-01,\n",
      "         -3.3073e-01,  6.1263e-01, -2.3439e-01, -6.6129e-01,  1.2815e+00,\n",
      "         -1.1285e+00, -5.2924e-01,  6.4357e-01, -2.8882e-01, -1.3075e+00,\n",
      "         -7.3547e-03, -1.2007e+00,  7.0737e-01, -6.1690e-01, -1.2554e+00,\n",
      "         -7.9636e-02, -6.7596e-01, -1.0027e+00, -7.1570e-01,  2.3428e+00,\n",
      "         -1.9793e+00, -1.1671e-01,  4.5135e-01,  5.2147e-01, -3.3362e+00,\n",
      "         -5.8253e-01,  1.4457e+00,  1.4814e+00, -3.4673e-01,  2.4567e-01,\n",
      "          5.2644e-01,  5.4547e-01,  2.1103e-02, -8.7463e-02,  1.3067e+00,\n",
      "         -1.2749e-01, -7.5148e-01,  9.7427e-02,  1.2168e+00,  7.2101e-01,\n",
      "         -1.5341e+00, -3.6981e-01, -2.0384e-01,  4.9770e-01, -1.1775e-01,\n",
      "          8.9499e-01,  4.4429e-01, -1.1500e+00, -2.0720e+00, -2.0004e-02,\n",
      "         -9.3268e-01,  1.6597e+00,  5.6015e-01, -9.9275e-01,  4.5506e-01,\n",
      "         -9.2094e-01, -1.0712e+00, -1.3234e+00, -4.7606e-01,  4.1406e-01,\n",
      "          4.9957e-02,  1.1547e+00, -9.5795e-02, -8.9346e-01,  1.5639e+00,\n",
      "          2.5333e-01, -2.4373e-01, -1.6420e+00, -5.4588e-01,  1.2899e+00,\n",
      "          9.8062e-01,  3.0028e-01, -9.2276e-01, -4.1132e-01,  5.8026e-01,\n",
      "          3.5982e-01,  3.7123e-01,  1.0468e+00, -1.0052e+00,  3.4026e-01,\n",
      "         -2.0370e+00, -1.0824e+00, -2.0365e+00,  5.9097e-02,  4.2667e-01,\n",
      "         -4.6629e-01, -1.5449e+00,  3.2303e-01, -1.2665e-02,  1.9481e+00,\n",
      "         -5.6180e-01,  1.6058e+00,  1.4943e-01, -4.2079e-01,  5.3664e-01,\n",
      "         -1.0382e+00, -1.2932e+00, -3.1812e-01,  1.0300e+00, -1.3530e+00,\n",
      "          8.4270e-01,  8.1920e-01,  1.0880e+00, -6.7732e-01,  2.5311e-01,\n",
      "          6.7181e-01,  4.4945e-01,  1.2186e+00,  2.7437e+00,  4.3178e-01,\n",
      "         -8.8379e-02, -9.6655e-01,  1.9387e+00, -6.0826e-01,  1.9811e+00,\n",
      "          1.0474e+00,  1.1530e+00,  1.2858e+00,  1.0660e+00, -6.5872e-02,\n",
      "          2.2462e-01, -1.3449e+00,  2.2514e-01,  2.2763e-01,  1.5520e+00,\n",
      "         -1.3481e+00, -1.1656e-01,  3.9838e-01, -7.6373e-01, -1.4973e+00,\n",
      "         -1.5040e+00,  5.6668e-01, -1.4463e+00,  1.9531e-02, -6.1435e-01,\n",
      "          1.1510e+00, -6.0539e-01,  4.2155e-01,  3.3432e-02, -4.8480e-01,\n",
      "         -8.6058e-01, -6.3660e-01,  1.1264e-01, -7.9633e-01,  3.2733e-01,\n",
      "         -8.4311e-01,  5.1068e-01,  1.0633e+00, -8.2689e-01,  1.5091e-01,\n",
      "         -6.9696e-01, -3.1741e-01,  3.9412e-01,  8.1834e-01,  9.9969e-01,\n",
      "          8.9395e-01, -1.4057e+00, -1.1852e+00, -5.9810e-01,  5.6631e-01,\n",
      "         -1.0778e+00,  2.2879e-01,  7.6280e-01,  5.8487e-02,  6.4148e-01,\n",
      "         -9.1335e-01, -1.5787e-01,  6.4098e-01,  1.5302e+00,  1.0426e+00,\n",
      "          8.3128e-01, -1.9019e-01, -3.3710e-01,  2.0027e+00,  1.0700e+00,\n",
      "         -6.7436e-01, -4.8529e-01,  6.0500e-01,  4.0147e-01,  2.0379e+00,\n",
      "          6.4584e-01, -1.1217e+00,  1.5420e+00,  1.1011e+00, -3.0307e-01,\n",
      "          6.9661e-01,  5.0940e-01, -4.5367e-01, -1.4468e+00,  9.7235e-01,\n",
      "         -1.7385e+00, -2.3061e+00, -1.2773e+00, -1.4245e+00, -4.4341e-01,\n",
      "         -1.7027e+00, -8.6636e-01,  3.0286e-01,  1.8372e-01,  8.8100e-01,\n",
      "          1.5984e+00,  1.4624e+00,  1.0796e+00,  6.1714e-01, -2.6732e-01,\n",
      "         -2.1797e-01,  1.0895e+00,  1.1664e-01, -1.3794e+00,  8.2327e-01,\n",
      "         -1.2316e+00, -4.1367e-01,  7.8619e-01,  3.4196e-01, -2.9828e+00,\n",
      "         -8.2175e-01,  2.3681e+00, -1.9688e+00,  5.5511e-01, -2.9529e-01,\n",
      "          1.8194e+00,  7.3544e-01, -1.4714e+00, -8.4383e-01,  1.3068e+00,\n",
      "          1.4646e+00,  3.5832e-01,  5.1543e-01,  6.5938e-01,  1.8802e+00,\n",
      "          8.1404e-01,  1.2127e+00,  3.1979e-01,  1.4628e+00,  1.5683e+00,\n",
      "         -2.4760e-01, -7.5073e-03, -1.7716e+00, -4.7079e-01, -9.4714e-01,\n",
      "          1.5001e-01,  1.7641e-01, -8.5675e-01,  3.0392e-01, -1.5199e-01,\n",
      "          6.1482e-01, -7.3988e-01,  9.6527e-02, -2.2787e-01, -5.2551e-01,\n",
      "         -5.9891e-01,  1.2837e-01,  1.2451e-02, -2.7003e-01, -3.4857e-01,\n",
      "          2.8065e-01, -4.5615e-01, -6.5746e-01, -1.0697e+00, -1.6966e+00,\n",
      "         -2.0107e-01, -5.6659e-01, -1.1503e+00, -2.1253e+00,  1.8677e+00,\n",
      "         -7.5360e-01, -1.1706e+00, -7.0560e-01, -1.9421e-01, -1.9739e-01,\n",
      "          1.1316e-01, -1.5155e-01,  1.6692e+00, -1.8645e+00,  3.3647e-01,\n",
      "         -1.6858e-01,  1.2512e+00,  9.9474e-01,  1.4522e-01,  4.0421e-02,\n",
      "          2.9886e-01,  2.2982e+00, -9.0631e-01, -2.1637e+00,  2.0552e+00,\n",
      "          1.2602e-01,  9.5770e-01,  1.4369e+00, -3.9093e-01, -1.3687e-01,\n",
      "          9.1525e-01, -1.8409e+00,  2.3598e-01,  2.6866e-01, -1.9144e+00,\n",
      "          1.1340e+00,  4.3649e-01,  3.4090e-01,  1.0500e+00, -4.0765e-01,\n",
      "          7.5494e-01, -1.9023e-01, -1.9724e+00, -2.6950e-01, -1.1963e+00,\n",
      "          3.1792e-01,  4.8521e-01, -2.9486e-01,  3.1041e-01,  3.4323e-01,\n",
      "          2.7313e-02,  9.5970e-01, -2.4031e-01,  7.3712e-01,  1.6600e+00,\n",
      "         -1.5637e+00, -5.8560e-01, -1.6279e+00,  6.4978e-01,  4.2163e-01,\n",
      "          1.1321e+00, -4.9425e-01,  2.6117e-01, -6.1392e-01, -2.5305e-01,\n",
      "         -6.2668e-02, -3.4419e-01,  1.0810e+00, -8.6975e-02,  9.0108e-01,\n",
      "         -1.4056e+00, -3.4871e-01, -1.1685e+00,  9.9406e-01, -2.0063e+00,\n",
      "          4.3471e-01, -5.7570e-01,  1.6334e+00, -3.1175e-01,  1.4521e+00,\n",
      "         -2.0978e-01,  1.8378e+00, -5.0055e-01,  1.6855e-01, -4.7591e-01,\n",
      "          1.8880e+00, -1.6117e+00,  7.4895e-01, -2.5854e-01,  1.3601e+00,\n",
      "         -1.1049e+00, -2.9869e-01, -2.8827e-01, -1.5737e+00,  4.4519e-01,\n",
      "         -1.5166e+00, -1.4750e+00, -8.1003e-01, -3.6813e-01,  1.3322e+00,\n",
      "         -4.6017e-01, -1.6954e-01, -4.8599e-01,  4.8695e-01, -2.3997e-01,\n",
      "          9.6130e-03,  1.2175e-01,  9.4600e-01, -1.7723e+00,  7.2234e-01,\n",
      "          3.3798e-01, -1.3779e-02, -1.8260e+00,  1.0970e+00, -1.0825e-01,\n",
      "          6.3754e-01,  2.1860e-01,  1.0861e+00, -5.7925e-01,  8.0035e-01,\n",
      "         -2.6972e+00, -2.1532e+00,  6.4651e-02,  1.0038e+00, -2.1515e-03,\n",
      "          1.8939e-01, -3.9398e-01, -4.0921e-01,  1.5114e+00, -5.0735e-01,\n",
      "         -1.0303e+00,  4.8125e-01, -2.8775e-01, -7.2037e-02, -1.1408e+00,\n",
      "         -5.2290e-01, -2.5404e-01,  1.8272e+00, -2.9765e-01,  1.5054e+00,\n",
      "         -8.5161e-01,  1.8005e-02, -1.2560e+00, -2.3441e-01,  3.0322e-01,\n",
      "          2.7965e+00,  7.9259e-01,  6.6010e-02,  2.1071e+00, -9.3788e-01,\n",
      "          1.3671e+00,  8.0020e-01, -1.0126e+00,  6.7755e-01, -1.0774e+00,\n",
      "         -4.8409e-01, -7.0908e-01,  3.1931e+00,  8.5246e-01,  2.3518e-01,\n",
      "          7.7246e-01,  1.2627e+00, -6.1111e-01,  1.1986e+00, -4.2009e-01,\n",
      "         -5.9261e-01,  1.4342e+00, -8.5974e-01,  1.0317e+00, -3.8692e-01,\n",
      "          1.0330e+00, -4.6329e-01, -1.6281e-02,  8.6101e-01, -1.5553e-01,\n",
      "          1.4677e+00,  5.2765e-02,  8.2930e-01,  2.9648e-02,  3.5141e-01,\n",
      "         -1.0237e+00, -4.6214e-01,  1.2918e-01, -1.5976e-02, -1.2662e+00,\n",
      "         -1.6724e-02, -7.7759e-02,  5.9853e-01,  4.0114e-01,  8.5837e-01,\n",
      "         -9.2587e-01,  9.3784e-01, -1.2348e+00,  2.2385e-01,  9.2294e-01,\n",
      "         -8.6143e-01,  6.6714e-01, -5.5054e-02,  6.7271e-01, -1.4862e+00,\n",
      "          4.6037e-01, -2.2388e-01, -2.1333e+00, -2.4284e-01,  1.8335e-01,\n",
      "          5.0966e-01, -2.2736e-02,  6.3896e-01, -6.7542e-01, -1.0421e+00,\n",
      "         -1.1953e+00,  6.3361e-01, -3.3508e-01,  8.8222e-01, -5.3171e-01,\n",
      "          1.6200e+00,  8.6670e-03, -1.0769e+00, -1.3194e+00,  1.4928e+00,\n",
      "         -1.6651e+00, -1.3304e+00, -2.8915e-02,  7.0007e-01, -1.8513e-01,\n",
      "         -6.9945e-01,  1.6693e+00,  1.6501e+00,  3.4851e-01,  1.0055e+00,\n",
      "         -3.9650e-01,  1.8564e-01, -1.0571e-01, -1.5438e+00, -1.1955e+00,\n",
      "         -9.6690e-01, -7.4834e-01,  1.0093e+00,  1.2733e-01, -2.6225e-01,\n",
      "          1.2675e-01, -1.3183e+00, -4.8242e-01,  1.8306e-01,  9.4666e-02,\n",
      "         -8.3298e-02,  1.2258e+00, -9.2357e-01, -5.1633e-01, -6.5547e-01,\n",
      "         -4.1394e-01,  1.0814e+00,  1.6808e-01,  9.3494e-01,  3.3142e-02,\n",
      "          1.4709e-02,  1.4568e-01,  2.6071e-01, -2.5343e-01,  9.0793e-01,\n",
      "         -1.1646e+00, -4.9480e-01,  2.9373e-02,  1.5916e+00, -5.6563e-01,\n",
      "         -4.9245e-01,  1.1221e+00, -9.0703e-01, -1.6487e-01,  1.5788e-01,\n",
      "         -4.5183e-01,  5.5937e-01, -3.4749e-01, -9.3839e-01, -8.0630e-01,\n",
      "         -1.6969e-01, -6.8712e-01,  1.9867e-02, -1.0617e+00, -2.9819e-01,\n",
      "          4.0155e-01, -5.7639e-01,  4.4724e-02, -6.7004e-01, -1.4341e+00,\n",
      "         -1.0354e+00,  6.8770e-01, -1.3993e+00, -7.2826e-01]])\n",
      "CrypTen bias: tensor([-1.3140])\n"
     ]
    }
   ],
   "source": [
    "#Finally, we decrypt the weights\n",
    "print(\"CrypTen weights:\", w_enc.get_plain_text())\n",
    "print(\"CrypTen bias:\", b_enc.get_plain_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.37%\n"
     ]
    }
   ],
   "source": [
    "# Let's examine our accuracy on the test data\n",
    "w_final = w_enc.get_plain_text()\n",
    "b_final = b_enc.get_plain_text()\n",
    "test_flattened = data_test_norm.flatten(start_dim=1)\n",
    "targets = mnist_test.targets.float()\n",
    "\n",
    "#compute output\n",
    "output = w_final.matmul(test_flattened.t()) + b_final\n",
    "output_sign = output.sign()\n",
    "\n",
    "#compute accuracy of output\n",
    "output_target = output_sign*targets\n",
    "correct = (output_target + 1).mul(0.5).sum().float()\n",
    "accuracy = correct/targets.size(0) * 100\n",
    "print(\"Test Accuracy: %.2f%%\" % accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, Alice and Bob may only need the labels of the test data in plaintext. In this situation, we would not need to decrypt ```w_enc``` and ```b_enc```. Instead, we could encrypt the the test data, and use the encrypted classifier (i.e., with ```w_enc``` and ```b_enc```) to classify the encrypted test data. The labels we get will be encrypted, and only these we would need to decrypt. The trained classifier itself remains encrypted.  \n",
    "\n",
    "There is one final item to understand. As we did in the earlier tutorials, we have used ```get_plain_text``` to decrypt the ```CrypTensors```. For this function to succeed, all the parties have to communicate their secret shares in order to carry out the decryption. Thus, the ```CrypTensors``` can only be decrypted if Alice and Bob agree to do so. \n",
    "\n",
    "### Application 2: Data Augmentation\n",
    "Next, we'll show how we can use CrypTen in the <i>Data Augmentation</i> application. Here Alice and Bob each have some examples, and would like to learn a classifier over their combined examples. As before, Alice and Bob wish to keep their respective data private. \n",
    "\n",
    "The steps we take are very similar to the <i>Feature Aggregation</i> application: (a) initialize each process with its data and dummy input, (b) encrypt the data, (c) concatenate the data, and (d) learn on encrypted tensors. Indeed, the main difference comes in Step (c), where the concatenation of the ```CrypTensors``` is done along the batch dimension.\n",
    "\n",
    "Let's walk through the first few steps to make this clear. We'll assume that because Alice and Bob each have part of the examples, they will also have only the corresponding part of the labels. Thus, we'll encrypt the labels and combine the encrypted labels as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data_alice and data_bob\n",
    "data_alice = mnist_train.data[:20000,:,:]\n",
    "data_bob = mnist_train.data[20000:,:,:]\n",
    "\n",
    "#Define labels_alice and labels_bob\n",
    "labels_alice = mnist_train.targets[:20000]\n",
    "labels_bob = mnist_train.targets[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step (a): Load each party's data into their process\n",
    "rank = comm.get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    #load Alice's data\n",
    "    x_alice = data_alice\n",
    "    y_alice = labels_alice\n",
    "else:\n",
    "    #load dummy input with the same shape\n",
    "    x_alice = torch.empty(data_alice.size())\n",
    "    y_alice = torch.empty(labels_alice.size())\n",
    "    \n",
    "    \n",
    "#Similarly, for Bob's data:\n",
    "if rank == 1:\n",
    "    #load Bob's data\n",
    "    x_bob = data_bob \n",
    "    y_bob = labels_bob\n",
    "else:\n",
    "    #load dummy input\n",
    "    x_bob = torch.empty(data_bob.size())\n",
    "    y_bob = torch.empty(labels_bob.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step (b): Encrypt the data\n",
    "x_alice_enc = crypten.cryptensor(x_alice, src=0)\n",
    "y_alice_enc = crypten.cryptensor(y_alice, src=0)\n",
    "\n",
    "x_bob_enc = crypten.cryptensor(x_bob, src=1)\n",
    "y_bob_enc = crypten.cryptensor(y_bob, src=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Alice's encrypted data:\n",
      "  Examples:  torch.Size([20000, 28, 28])  Labels: torch.Size([20000])\n",
      "Size of Bob's encrypted data:\n",
      "  Examples:  torch.Size([40000, 28, 28])  Labels: torch.Size([40000])\n",
      "\n",
      "Size of the combined data:\n",
      "  Examples:  torch.Size([60000, 28, 28])  Labels: torch.Size([60000])\n",
      "Combined data:\n",
      "  Examples encrypted: True \n",
      "  Labels encrypted: True\n"
     ]
    }
   ],
   "source": [
    "#Step (c): Create the combined encrypted data\n",
    "print(\"Size of Alice's encrypted data:\\n\", \" Examples: \", x_alice_enc.size(), \" Labels:\", y_alice_enc.size()) \n",
    "print(\"Size of Bob's encrypted data:\\n\", \" Examples: \", x_bob_enc.size(), \" Labels:\", y_bob_enc.size())\n",
    "print()\n",
    "\n",
    "#Combine the examples and labels: concatenate along batch dimension\n",
    "x_combined_enc = crypten.cat([x_alice_enc, x_bob_enc], dim=0)\n",
    "y_combined_enc = crypten.cat([y_alice_enc, y_bob_enc], dim=0)\n",
    "\n",
    "print(\"Size of the combined data:\\n\", \" Examples: \", x_combined_enc.size(), \" Labels:\", y_combined_enc.size())\n",
    "print(\"Combined data:\\n\", \" Examples encrypted:\", crypten.is_encrypted_tensor(x_combined_enc), \n",
    "      \"\\n  Labels encrypted:\", crypten.is_encrypted_tensor(y_combined_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step (c) contains only main difference from the <i>Feature Aggregation</i> application. Here we concatenated the data along the batch dimension (```dim 0```), while in <i>Feature Aggregation</i>, we used the feature dimension (```dim 1```). \n",
    "\n",
    "We can now train with this data exactly as we did earlier, in Step (d).\n",
    "\n",
    "This completes our tutorial on access control in CrypTen in the context of two common applications."
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "2421368098141808"
  },
  "disseminate_notebook_info": {
   "bento_version": "20190826-030256",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/shobha/fbsource/fbcode/bento/kernels/local/cryptenk/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "138739",
   "others_can_edit": true,
   "reviewers": "",
   "revision_id": "655907724900490",
   "tags": "",
   "tasks": "",
   "title": "Tutorial 3 -- Introduction to Access Control"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
