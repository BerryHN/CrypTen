{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to Access Control\n",
    "\n",
    "We can now start using CrypTen to carry out private computations in some common use cases. In this tutorial, we will look at the first two use cases described in the Introduction, <i>Feature Aggregation</i> and <i>Data Augmentation</i>. In both use cases, we'll use a simple two-party setting and demonstrate how we can learn a linear SVM. In the process, we will see how access control works in CrypTen. We'll return to creating `CrypTensors` with the high-level `crypten.cryptensor` factory function, as we did in Tutorial 1.\n",
    "\n",
    "As usual, we'll begin by importing the `crypten` and `torch` libraries, and initialize `crypten` with `crypten.init()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import torch\n",
    "\n",
    "crypten.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application 1: Feature Aggregation\n",
    "\n",
    "In this scenario, two parties, Alice and Bob, each have a part of the features of the dataset. We'll use the MNIST data to demonstrate how Alice and Bob can learn without revealing protected information. For reference, the feature size of each example in the MNIST data is `28 x 28`, and there are 60000 examples in training data and 10000 examples in the test data. \n",
    "\n",
    "Let's assume Alice has the first `28 x 20` features and Bob has last `28 x 8` features. One way to think of this split is that Alice has the (roughly) top 2/3rds of each image, while Bob has the bottom 1/3rd of each image. We'll see how we can use CrypTen to learn over all `28 x 28` features (i.e., the entire image), while keeping each party's features private.\n",
    "\n",
    "For ease of use, we have created a helper script `mnist_utils.py` that downloads the publicly available MNIST data, and splits the data as required. We will first use this script to create a dataset such that Alice has the first `28 x 20` features of each example, and Bob has last `28 x 8` features of each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../examples/mnist_utils.py --option features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 20])\n",
      "torch.Size([10000, 28, 20])\n"
     ]
    }
   ],
   "source": [
    "data_alice_train = crypten.load('/tmp/alice_train.pth', src=0)\n",
    "print(data_alice_train.size())\n",
    "\n",
    "data_alice_test = crypten.load('/tmp/alice_test.pth', src=0)\n",
    "print(data_alice_test.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll illustrate how Alice and Bob learn privately in 4 steps: (a) loading the data, (b) encrypting the data, (c) constructing the encrypted training data, and (d) training privately. \n",
    "\n",
    "### Step (a): Loading the data\n",
    "\n",
    "We'll begin by loading Alice's data and Bob's data. \n",
    "\n",
    "Before we do so, we need to understand a little more about how CrypTen is implemented. CrypTen runs a separate process for each party, but each process runs the identical (complete) program. We therefore need a mechanism to ensure that each process holds its data, and shares only the encrypted version with the other processes. \n",
    "\n",
    "As is standard in MPI programming, CrypTen uses a `rank` variable to identify the process (and thus the party). Let's assume Alice has the `rank` 0 process and Bob has the `rank` 1 process. When loading the data, we have to provide the `CrypTensor` with the source rank, i.e., rank of the process that would hold its data. This is provided through the `src` keyword in the `crypten.cryptensor` load function. \n",
    "\n",
    "Let's look at an example now. We'll load both parties' data, and then we'll look at the sizes of the data tensors they hold. Again, both processes run all the code, so both Alice and Bob will create both data tensors. However, the `src` keyword tells the process whether to load the data from the file, or to load an empty tensor of the appropriate shape (this is a requirement of `torch.distributed`, our communication backend).   \n",
    "\n",
    "<i><small>(Technical note: Because Jupyter notebooks run only a single process, we simulate a multi-party world with the `@mpc.run_multiprocess` decorator.)</small></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 Alice data size: torch.Size([60000, 28, 20])\n",
      "Rank 0 Alice data size: torch.Size([60000, 28, 20])\n",
      "Rank 0 Bob data size: torch.Size([60000, 28, 8])\n",
      "Rank 1 Bob data size: torch.Size([60000, 28, 8])\n"
     ]
    }
   ],
   "source": [
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def load_private_data():\n",
    "    \n",
    "    #Load Alice's data with src=0 since Alice has the rank 0 process\n",
    "    x_alice = crypten.load('/tmp/alice_train.pth', src=0)\n",
    "    \n",
    "    #Load Bob's data with src=1 since Bob has the rank 1 process\n",
    "    x_bob = crypten.load('/tmp/bob_train.pth', src=1)\n",
    "    \n",
    "    #Check the sizes of the data for each process\n",
    "    rank = comm.get().get_rank() #We can access the rank of the process through this function\n",
    "    print(f\"Rank {rank} Alice data size: {x_alice.size()}\")\n",
    "    print(f\"Rank {rank} Bob data size: {x_bob.size()}\")\n",
    "    \n",
    "z = load_private_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step (b): Encrypting the data\n",
    "\n",
    "Next, we encrypt the data by creating `CrypTensors`, just as we did in Tutorial 1. But here there is one crucial difference: we have to provide the same `src` of the data to the `crypten.cryptensor` function that we did to the `crypten.load` function. In our example, when creating `CrypTensor` for Alice's data, we should use `src=0`; when creating `CrypTensor` for Bob's data, we should use `src=1`. \n",
    "\n",
    "<i><small>(Technical note: As we mentioned before, because Jupyter notebooks run only a single process, we simulate a multi-party world with the `@mpc.run_multiprocess` decorator. However, as a result, the variables loaded do not carry over from cell to cell as is customary in a notebook. Therefore, we reinitialize `x_alice` and `x_bob` in each cell simulating a multi-party world.)</small></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def construct_encrypted_data():\n",
    "\n",
    "    #Load the data\n",
    "    #Load Alice's data with src=0\n",
    "    x_alice = crypten.load('/tmp/alice_train.pth', src=0)\n",
    "    #Load Bob's data with src=1\n",
    "    x_bob = crypten.load('/tmp/bob_train.pth', src=1)\n",
    "\n",
    "    #Encrypt the data: \n",
    "    #Alice's data gets encrypted with src=0\n",
    "    x_alice_enc = crypten.cryptensor(x_alice, src=0)\n",
    "    #Bob's data gets encrypted with src=1\n",
    "    x_bob_enc = crypten.cryptensor(x_bob, src=1)\n",
    "    \n",
    "z = construct_encrypted_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both the rank 0 and the rank 1 process call the `crypten.cryptensor` on both `x_alice` and `x_bob` tensors. However, because of the `src` argument of `crypten.cryptensor`, the rank 0 process and the rank 1 process perform different actions on both `x_alice` and `x_bob`. For `x_alice`, the rank 0 process will construct secret shares of `x_alice`, and provide a share to the rank 1 process; `x_alice_enc` will contain Alice's secret share for process 0 and Bob's secret share for process 1. For `x_bob`, the rank 1 process will construct secret shares of `x_alice`, and provide a share to the rank 1 process; `x_bob_enc` will contain Alice's secret share for process 0 and Bob's secret share for process 1. \n",
    "\n",
    "\n",
    "### Step (c): Constructing the Encrypted Training Data\n",
    "To use both Alice's features and Bob's features for training, we'll construct a tensor that concatenates both encrypted tensors. We'll do this with CrypTen's `cat` function, similar to `torch.cat`. This creates a new `CrypTensor`.\n",
    "\n",
    "<i><small>(Technical note: Again, when using the `@mpc.run_multiprocess` decorator, the variables loaded do not carry over from cell to cell as is customary in a notebook. Therefore, we reinitialize `x_alice`, `x_bob`, `x_alice_enc` and `x_bob_enc` in the following cell.)</small></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1 Size of Alice's encrypted data: torch.Size([60000, 28, 20])\n",
      "Rank: 1 Size of Bob's encrypted data: torch.Size([60000, 28, 8])\n",
      "\n",
      "Rank: 0 Size of Alice's encrypted data: torch.Size([60000, 28, 20])\n",
      "Rank: 0 Size of Bob's encrypted data: torch.Size([60000, 28, 8])\n",
      "\n",
      "Rank: 1 Size of the combined data: torch.Size([60000, 28, 28])\n",
      "Rank: 1 Combined data encrypted: True\n",
      "Rank: 0 Size of the combined data: torch.Size([60000, 28, 28])\n",
      "Rank: 0 Combined data encrypted: True\n"
     ]
    }
   ],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def construct_and_combine_encrypted_data():\n",
    "    x_alice = crypten.load('/tmp/alice_train.pth', src=0)\n",
    "    x_bob = crypten.load('/tmp/bob_train.pth', src=1)\n",
    "\n",
    "    x_alice_enc = crypten.cryptensor(x_alice, src=0)\n",
    "    x_bob_enc = crypten.cryptensor(x_bob, src=1)\n",
    "    \n",
    "    rank = comm.get().get_rank()\n",
    "    print(f\"Rank: {rank} Size of Alice's encrypted data: {x_alice_enc.size()}\") \n",
    "    print(f\"Rank: {rank} Size of Bob's encrypted data: {x_bob_enc.size()}\")\n",
    "    print()\n",
    "\n",
    "    #using crypten.cat to combine the feature sets\n",
    "    x_combined_enc = crypten.cat([x_alice_enc, x_bob_enc], dim=2)\n",
    "    \n",
    "    print(f\"Rank: {rank} Size of the combined data: {x_combined_enc.size()}\")\n",
    "    print(f\"Rank: {rank} Combined data encrypted: {crypten.is_encrypted_tensor(x_combined_enc)}\")\n",
    "    \n",
    "z = construct_and_combine_encrypted_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not reveal any private information by doing so: process 0 will construct a tensor that concatenates Alice's shares of `x_alice_enc` and `x_bob_enc`, and process 1 will construct a tensor that concatenates Bob's shares of `x_alice_enc` and `x_bob_enc`. \n",
    "\n",
    "We can now use this data to train in CrypTen just as we would use plaintext data in PyTorch. \n",
    "\n",
    "### Step (d): Training with Encrypted Data \n",
    "We'll now use a linear SVM classifier to show how CrypTen can train on encrypted data. CrypTen implements all of the necessary operations required for this (and many other) learning algorithms to operate on encrypted tensors, so we can implement the learning in the same way as we would on plaintext tensors. \n",
    "\n",
    "The code below implements the learning algorithm in CrypTen. While each step is carried out on ```CrypTensors```, the learning algorithm looks just as it would in PyTorch! The only difference is that, in CrypTen, the learned weights and bias are ```CrypTensors```. If the plaintext versions of weights and bias are required, Alice and Bob will have to agree to decrypt them at the end of the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is required for demonstrating the learning algorithm in our notebook. \n",
    "# As Jupyter notebooks run only a single process, Alice and Bob both need to encrypt with \n",
    "# src=0 in order for the remaining code to run. In a regular CrypTen implementation \n",
    "# (see the CrypTen examples folder), x_enc_bob would be encrypted with src=1 as shown in the cells above.\n",
    "x_alice = crypten.load('/tmp/alice_train.pth', src=0)\n",
    "x_alice_enc = crypten.cryptensor(x_alice, src=0)\n",
    "\n",
    "#The following two lines would use src=1 when run outside a Jupyter notebook.\n",
    "x_bob = crypten.load('/tmp/bob_train.pth', src=0)\n",
    "x_bob_enc = crypten.cryptensor(x_bob, src=0) \n",
    "\n",
    "x_combined_enc = crypten.cat([x_alice_enc, x_bob_enc], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load labels\n",
    "label_train = crypten.load('/tmp/train_labels.pth')\n",
    "label_test = crypten.load('/tmp/test_labels.pth')\n",
    "\n",
    "# Modify the labels so that:\n",
    "# all non-zero digits have class label 1.\n",
    "# all zero digits have class label -1\n",
    "label_train[label_train == 0] = -1\n",
    "label_train[label_train != 0] = 1\n",
    "label_test[label_test == 0] = -1\n",
    "label_test[label_test != 0] = 1\n",
    " \n",
    "#We'll use only the first 10k examples so it runs faster\n",
    "data_enc = x_combined_enc[:10000,:,:]\n",
    "labels = label_train[:10000]\n",
    "examples = data_enc.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "--- Accuracy 86.25%\n",
      "Epoch 10\n",
      "--- Accuracy 94.56%\n",
      "Epoch 15\n",
      "--- Accuracy 96.56%\n",
      "Epoch 20\n",
      "--- Accuracy 97.51%\n",
      "Epoch 25\n",
      "--- Accuracy 98.03%\n",
      "Epoch 30\n",
      "--- Accuracy 98.37%\n",
      "Epoch 35\n",
      "--- Accuracy 98.59%\n",
      "Epoch 40\n",
      "--- Accuracy 98.75%\n",
      "Epoch 45\n",
      "--- Accuracy 98.87%\n",
      "Epoch 50\n",
      "--- Accuracy 99.02%\n"
     ]
    }
   ],
   "source": [
    "# Random initialization for linear svm\n",
    "w_init = torch.randn(1, 28*28)\n",
    "b_init = torch.randn(1)\n",
    " \n",
    "# Turn all tensors into encrypted tensors\n",
    "y_enc = crypten.cryptensor(labels)   \n",
    "w_enc = crypten.cryptensor(w_init)\n",
    "b_enc = crypten.cryptensor(b_init)\n",
    "\n",
    "#define parameters: epoch and learning rate\n",
    "epochs = 50\n",
    "lr = 0.1\n",
    "log_accuracy = True\n",
    "\n",
    "x_flatten_enc = data_enc.flatten(start_dim=1)\n",
    "\n",
    "for i in range(epochs):\n",
    "        # Forward\n",
    "        yhat = w_enc.matmul(x_flatten_enc.t()) + b_enc\n",
    "        yhat = yhat.sign()\n",
    "\n",
    "        yy = yhat * y_enc\n",
    "\n",
    "        if log_accuracy and i%5 == 4:\n",
    "            # Compute accuracy\n",
    "            correct = (yy + 1).mul(0.5).sum()\n",
    "            print(\"Epoch %d\" % (i + 1))\n",
    "            print(\n",
    "                \"--- Accuracy %.2f%%\"\n",
    "                % (correct.get_plain_text().float().div(examples).item() * 100)\n",
    "            )\n",
    "        # Backward\n",
    "        loss_grad = y_enc * (yy - 1) * 0.5\n",
    "\n",
    "        b_grad = loss_grad.sum()/examples\n",
    "        w_grad = loss_grad.matmul(x_flatten_enc)/examples\n",
    "\n",
    "        # Update\n",
    "        w_enc = w_enc - w_grad * lr\n",
    "        b_enc = b_enc - b_grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrypTen weights: tensor([[ 5.7780e-01,  1.2894e+00, -8.7309e-01, -6.1832e-01,  1.1711e+00,\n",
      "          4.5486e-02, -1.8327e+00,  1.8540e+00,  3.9236e-01, -8.5716e-01,\n",
      "          1.1499e+00,  1.2826e+00, -6.0641e-01,  7.0221e-02, -3.0348e-01,\n",
      "         -4.7008e-01, -2.2560e-01,  1.0942e+00,  8.7486e-01,  1.7632e+00,\n",
      "         -1.1922e+00, -8.2465e-01, -1.3471e+00, -7.1735e-01,  1.9817e+00,\n",
      "         -3.6327e-01,  1.1384e+00, -2.0858e+00,  1.0813e+00, -1.3672e+00,\n",
      "         -2.1877e-01,  7.3682e-01,  1.1379e+00,  5.3615e-01, -1.2881e+00,\n",
      "         -5.1572e-01, -1.8575e+00,  2.9251e-02,  5.3888e-01, -2.5453e+00,\n",
      "         -9.8712e-01, -4.5172e-01, -1.1684e+00, -2.3492e+00,  2.2895e+00,\n",
      "         -1.2922e+00,  4.8813e-01,  9.8215e-01,  4.1728e-01, -4.9069e-01,\n",
      "          7.9715e-01, -1.7189e-01, -5.3986e-02,  1.6958e+00, -7.4800e-01,\n",
      "          9.4496e-01, -1.1786e-01,  1.8239e+00,  6.6832e-01, -2.1777e+00,\n",
      "         -7.7705e-01,  1.2912e-01, -5.0041e-01,  8.6090e-01, -1.6803e+00,\n",
      "         -5.8594e-03,  1.4705e-01,  4.8645e-02, -1.0492e+00, -3.3325e-02,\n",
      "         -1.2417e+00, -1.8669e-01,  7.2766e-01,  1.9646e-01,  1.0824e+00,\n",
      "          1.6469e+00,  4.3930e-01, -4.9731e-01, -8.5672e-01,  1.0923e+00,\n",
      "          4.2929e-01,  4.8705e-01,  2.0489e+00, -6.9507e-01,  1.1757e+00,\n",
      "         -1.4864e-01, -7.4908e-01, -9.8984e-02, -1.4920e-01, -9.6069e-02,\n",
      "         -1.0882e+00, -4.7916e-01, -2.5070e-01,  1.2737e+00, -2.2884e-01,\n",
      "         -2.4576e+00, -5.0432e-01,  2.8983e-01,  1.7949e+00,  1.2081e+00,\n",
      "         -1.2851e+00,  1.0057e+00, -3.4012e-01,  1.7355e-01, -3.2194e+00,\n",
      "          2.1042e+00,  3.6099e-01, -6.8005e-01,  4.7920e-01, -8.3389e-02,\n",
      "          1.2285e+00,  1.9131e-01, -1.6613e+00,  7.6515e-01, -1.7065e+00,\n",
      "          2.8255e-01,  6.1801e-01, -1.1925e+00, -1.1417e+00, -2.6033e-01,\n",
      "          1.5206e+00, -8.5596e-01,  1.0679e+00, -1.0709e+00,  8.2864e-01,\n",
      "          1.3026e-01, -1.2171e+00, -8.2056e-01,  8.7988e-01,  1.9374e+00,\n",
      "         -4.2966e-01, -2.9970e-01, -1.7748e+00, -4.2935e-01, -1.2443e+00,\n",
      "         -2.1567e-01,  4.9835e-02, -1.1914e-01,  1.1645e+00, -4.1890e-01,\n",
      "          1.4868e+00, -4.3243e-01, -1.1862e+00,  8.0145e-01, -2.3017e+00,\n",
      "          3.7045e-01,  3.9769e-01, -1.1964e+00, -6.0068e-01,  8.1551e-01,\n",
      "         -8.8953e-01, -1.3627e+00,  2.3454e-01, -1.2373e-01, -1.0481e+00,\n",
      "         -3.0623e-01,  6.4789e-02, -8.7715e-01,  1.9080e+00,  5.2716e-01,\n",
      "          4.0793e-01, -8.2774e-01, -3.7764e-01,  1.4810e-01, -3.0817e-01,\n",
      "         -1.8445e+00, -1.9040e-01,  4.8473e-01, -1.1877e+00,  1.4393e+00,\n",
      "          7.4286e-01, -1.9975e-01, -6.2007e-01, -3.9195e-01,  6.2973e-01,\n",
      "         -5.4631e-01,  1.1201e+00,  6.6956e-01, -1.4790e+00,  1.5340e+00,\n",
      "          1.3138e+00,  1.4552e-01,  1.6355e+00,  4.8515e-01, -8.6642e-01,\n",
      "          7.9346e-02, -1.9147e+00,  2.0421e-01, -9.5825e-02,  2.6086e-01,\n",
      "          2.3375e-01,  2.4982e-01,  9.3057e-01, -8.4715e-01,  1.1144e+00,\n",
      "          8.3981e-01, -7.6147e-01, -2.0968e+00, -2.7331e+00,  9.6268e-02,\n",
      "          1.6158e+00,  3.2135e-01, -4.9118e-02,  2.5031e-01, -2.2400e-01,\n",
      "          7.3306e-01,  8.1744e-01,  1.1242e+00, -7.9816e-01, -1.2614e+00,\n",
      "         -5.3711e-02,  9.4322e-01, -4.2838e-01,  6.7674e-01,  1.8875e+00,\n",
      "          1.0009e+00,  5.2252e-01,  1.3658e+00,  5.3409e-01, -1.8715e+00,\n",
      "          2.1324e-01,  5.7335e-01, -5.3923e-01, -2.1884e+00, -1.1385e-01,\n",
      "          1.6389e+00, -5.2371e-01,  1.2814e+00, -1.1961e-01,  2.1122e+00,\n",
      "         -1.1900e+00, -1.0032e+00,  1.8271e+00, -1.5408e+00, -1.5047e+00,\n",
      "          4.8811e-01, -2.9295e+00,  1.3505e+00,  4.0042e-01,  1.1161e+00,\n",
      "          1.2778e+00, -1.2112e+00, -2.8288e-01,  1.6198e+00, -8.6996e-01,\n",
      "          9.5398e-02, -5.8990e-02, -5.9190e-01,  2.9602e-02,  2.4358e-01,\n",
      "         -2.6044e-01,  4.4408e-01, -1.3383e+00,  6.2579e-01,  8.8351e-01,\n",
      "         -6.7123e-01,  1.0329e+00,  1.4597e+00,  5.3487e-01, -4.6893e-01,\n",
      "          7.7112e-01,  7.3592e-01,  3.1723e-01,  1.0086e+00,  9.9864e-01,\n",
      "          3.2153e-01, -1.7147e+00, -2.6833e-01,  6.0640e-01, -7.0938e-02,\n",
      "          3.9980e-01, -9.5631e-01, -1.2196e-01,  7.1011e-01, -1.1631e+00,\n",
      "         -3.0745e-01, -6.2454e-02, -6.8550e-01,  8.2529e-01, -2.0259e-01,\n",
      "         -1.1184e+00, -5.5049e-01, -1.6848e+00, -1.0526e+00,  7.4751e-01,\n",
      "         -4.2932e-01, -2.1044e+00,  1.6577e+00, -1.6097e+00,  2.1144e-01,\n",
      "         -1.3622e-01, -4.6915e-01,  9.4916e-01,  8.5907e-03, -7.2993e-01,\n",
      "          6.3641e-01,  4.6262e-01,  9.0189e-01,  1.7376e+00, -1.7455e-01,\n",
      "          3.0624e-01, -2.0589e-01,  8.0942e-01, -1.3254e-01, -4.6738e-02,\n",
      "         -5.0508e-01,  1.6348e-01,  2.4893e-01,  8.5931e-01,  5.0029e-01,\n",
      "         -1.7540e+00,  8.1288e-01, -1.0597e+00, -3.0598e-01,  1.6852e+00,\n",
      "         -1.5624e+00,  1.9255e-01,  1.3851e+00,  1.4603e+00, -8.9342e-01,\n",
      "          1.2875e-01, -4.9820e-02, -2.8648e-01,  1.1192e+00,  5.8273e-02,\n",
      "          1.3667e+00,  8.8135e-01,  4.8013e-01, -4.8776e-01, -1.3184e-01,\n",
      "          1.5723e+00,  4.5367e-01, -1.2650e+00, -5.7887e-01,  3.6539e-01,\n",
      "         -6.8405e-02, -1.6235e+00,  3.6011e-03,  2.3239e-01,  7.7484e-01,\n",
      "          1.8101e+00,  2.8659e-01, -4.4620e-01, -1.4502e+00, -3.7880e-01,\n",
      "          1.2947e+00, -1.2783e+00,  2.0507e+00,  6.2709e-01, -1.0765e+00,\n",
      "          1.5660e-01, -2.5197e+00, -1.0360e+00, -9.2390e-01,  1.0517e+00,\n",
      "         -2.6556e-01, -7.6770e-01, -7.6485e-01, -2.5985e+00, -5.8290e-01,\n",
      "         -1.1713e+00,  1.4456e-01, -1.2169e+00,  8.3755e-01, -1.8522e+00,\n",
      "          2.6115e-01, -4.4403e-03,  6.6252e-01,  1.2183e+00, -4.2372e-01,\n",
      "         -1.2076e-01,  4.7714e-02, -9.8906e-01, -9.4472e-01, -2.3651e-03,\n",
      "         -1.4832e-01,  6.4153e-01, -6.6003e-01, -2.9539e-01,  1.7283e+00,\n",
      "          9.5549e-01, -2.7267e+00,  1.6392e+00,  7.8661e-01,  5.6900e-02,\n",
      "         -2.1533e-01, -4.6347e-01,  1.2125e-01,  5.7480e-02,  2.4008e-01,\n",
      "         -1.2832e+00,  9.1422e-01,  1.9269e+00,  9.6713e-01, -9.2845e-01,\n",
      "         -3.7675e-01, -8.8908e-01,  8.0797e-01, -1.0439e+00,  1.4354e+00,\n",
      "          8.5655e-01,  2.0887e+00,  4.2520e-01, -6.7943e-01,  1.0422e+00,\n",
      "          1.1639e+00,  1.5576e-01, -1.9722e+00, -7.3531e-01,  2.1925e+00,\n",
      "          1.8425e+00,  9.6161e-02,  2.2087e+00, -3.4256e-02,  5.4446e-01,\n",
      "          1.5523e+00, -1.8303e+00,  8.6485e-01, -9.1290e-01,  2.1248e-01,\n",
      "         -9.2133e-02, -8.0063e-02,  6.7976e-01, -1.7365e+00, -8.9417e-01,\n",
      "         -1.1270e-01,  3.3885e-01,  8.0103e-01, -3.6157e-01,  1.4306e+00,\n",
      "         -6.0049e-01,  1.5320e+00,  1.1354e-01, -6.5207e-01,  9.3262e-01,\n",
      "          2.9506e-01,  5.3696e-01,  1.0658e-01,  1.9269e+00, -2.1691e+00,\n",
      "          6.0762e-01,  8.4842e-01,  7.7988e-01,  4.8717e-01, -2.1202e+00,\n",
      "          2.6729e-01,  2.8239e+00,  4.8610e-01, -2.2722e-01,  1.2787e-01,\n",
      "          2.2430e-02, -3.0479e-01, -1.5760e+00,  2.4801e+00, -1.2230e+00,\n",
      "          7.6190e-01,  4.9725e-01,  6.0553e-01,  1.7530e+00, -1.4409e+00,\n",
      "          9.2737e-01,  1.3562e-01,  2.3329e+00, -2.1507e-01, -4.7438e-01,\n",
      "          5.9534e-01, -1.2788e-01, -3.6615e-01,  2.8198e-01, -1.2674e+00,\n",
      "         -1.7937e-01, -4.7440e-02, -1.1690e+00, -4.7934e-01, -3.9671e-01,\n",
      "         -1.5587e+00, -2.0973e+00, -6.3419e-01, -1.6478e+00, -5.5917e-01,\n",
      "          1.2846e+00, -5.8440e-01,  4.1551e-01, -5.2086e-01,  1.1139e-03,\n",
      "          9.4943e-01, -1.7192e-01, -7.9414e-01,  2.4863e-01,  2.0082e-01,\n",
      "          3.7837e-01,  1.3300e+00, -7.4986e-01,  6.7564e-01, -4.0733e-01,\n",
      "          7.4321e-01, -9.4460e-01,  5.7126e-01, -1.1367e+00,  1.6105e+00,\n",
      "          7.7786e-01, -7.0882e-01,  2.2067e-01, -1.6632e+00, -6.7662e-01,\n",
      "          7.9228e-01,  4.7879e-01, -6.7476e-01,  4.4611e-01, -3.5814e-01,\n",
      "         -1.1903e+00,  9.9031e-01, -1.4239e+00, -1.9463e+00,  2.6683e-01,\n",
      "          4.1077e-01,  5.5670e-01,  2.3071e-02, -4.0192e-01, -1.1404e+00,\n",
      "          1.7242e-02,  1.1476e+00, -9.4025e-02, -6.6614e-01,  4.1740e-01,\n",
      "          2.2537e+00,  8.8445e-01, -1.8391e-01, -6.1511e-01, -2.4193e-01,\n",
      "          1.5289e+00, -1.7998e-01, -8.1772e-02,  1.6937e-01,  1.7299e+00,\n",
      "         -1.4322e+00,  1.3805e-01,  2.5986e-02,  1.4378e+00, -8.2074e-01,\n",
      "         -4.0375e-01,  8.7390e-01, -8.0495e-01,  2.1078e+00,  1.3668e+00,\n",
      "         -1.1049e+00,  5.8629e-01,  1.1082e-01,  1.7931e+00, -5.0186e-02,\n",
      "          1.4124e-01, -1.3179e+00,  1.7405e+00, -1.0832e+00, -1.4882e+00,\n",
      "         -5.5183e-01, -2.7977e-01, -7.9875e-01, -5.8569e-01, -4.4774e-01,\n",
      "          8.0940e-01, -5.5586e-01, -1.0303e-01, -6.1758e-01, -3.4764e-01,\n",
      "          6.0628e-01,  6.2410e-01,  6.3739e-01, -5.9842e-01, -5.8792e-01,\n",
      "         -6.9312e-01, -1.0449e+00, -1.0284e-01,  2.3483e-01,  1.0517e+00,\n",
      "         -4.1537e-01, -2.8259e-01,  1.7543e+00, -1.0167e+00, -9.8631e-01,\n",
      "         -4.5885e-01,  2.9652e-01,  6.5416e-01, -6.8719e-01, -8.3392e-01,\n",
      "          8.6932e-01, -2.1387e-01,  7.0328e-02,  8.7297e-01,  1.6740e-01,\n",
      "          6.1575e-01,  9.2429e-01,  5.8369e-01, -1.6777e+00,  4.8460e-01,\n",
      "          3.5576e-01,  4.2586e-01, -2.1362e-02,  8.3611e-01, -5.5145e-02,\n",
      "          1.1004e+00, -3.3720e-01,  1.3414e+00,  1.0356e+00,  1.7206e-01,\n",
      "          8.9235e-01,  3.2922e-01, -2.6041e-01, -2.0677e-01, -9.5267e-01,\n",
      "         -7.9893e-01, -1.2105e+00, -7.7750e-01,  3.2311e+00,  2.3935e-01,\n",
      "          8.1734e-01,  5.8575e-01, -5.3429e-01, -2.4708e+00,  1.4166e+00,\n",
      "         -7.1045e-01,  1.6937e-01, -1.2621e+00, -3.2990e-02,  1.4207e-01,\n",
      "         -1.7485e-01, -1.5297e+00,  8.8486e-02,  2.6886e-01,  2.3289e-01,\n",
      "          1.6009e+00,  2.7637e-01, -3.4348e-01, -1.2750e+00,  1.5062e-01,\n",
      "          7.2275e-01,  9.7614e-01, -2.4091e-01, -9.0810e-01,  1.3661e+00,\n",
      "          1.3939e+00,  6.6177e-01,  6.6466e-01,  1.0472e+00, -4.2957e-01,\n",
      "         -2.2044e+00, -1.1584e+00, -1.7664e+00,  4.4278e-01, -9.3330e-01,\n",
      "         -2.3524e-01, -2.7831e+00, -4.8355e-02,  9.6423e-01, -1.2927e+00,\n",
      "          1.5741e+00, -1.5417e+00,  7.7992e-01, -1.3107e+00,  1.5027e+00,\n",
      "          3.6104e-01, -7.4966e-02, -3.2013e-02,  5.1942e-01,  5.5063e-01,\n",
      "          3.1235e-02,  5.2553e-01,  8.7546e-01, -1.1171e+00,  1.5321e-01,\n",
      "          5.8638e-01, -2.1985e-01,  1.2626e+00,  6.4412e-01, -1.1265e+00,\n",
      "          1.5342e+00,  3.2808e-01, -3.6746e-01, -1.1579e+00,  1.5392e-01,\n",
      "          1.9179e+00,  2.5398e-01,  4.2545e-01, -5.2815e-01,  2.3041e-01,\n",
      "         -5.1868e-01,  4.4109e-01, -3.0193e-01,  1.1364e+00, -7.1732e-01,\n",
      "          7.6282e-01,  1.5836e+00,  2.1892e+00, -4.1324e-01,  2.4387e+00,\n",
      "         -6.5631e-01,  3.7125e-02, -1.6392e+00,  3.9674e-01, -1.2117e-01,\n",
      "         -6.1462e-01, -6.5297e-01,  4.1974e-01, -7.3253e-01,  1.9614e-01,\n",
      "         -2.0922e+00, -1.6198e+00,  6.7403e-01, -8.8388e-01,  1.6374e+00,\n",
      "          1.7887e+00, -1.0191e+00,  1.9119e-02, -5.4871e-01,  4.3640e-03,\n",
      "         -1.1980e-01,  8.0527e-01, -1.7185e+00, -6.5445e-02, -3.2744e-01,\n",
      "         -2.1814e-01,  1.4251e+00, -5.5371e-01,  8.5107e-01, -3.7801e-01,\n",
      "         -1.0780e+00, -1.3402e+00,  3.1268e-01, -3.3728e-01,  7.4127e-02,\n",
      "         -9.3607e-01, -5.6978e-01, -3.2588e+00, -1.0696e+00, -5.7439e-01,\n",
      "         -1.7319e-01,  2.2923e-01, -1.2782e+00, -1.1387e+00,  1.7477e+00,\n",
      "          9.4054e-01, -5.2499e-01, -9.0361e-01,  5.6120e-01,  9.5963e-02,\n",
      "         -3.6122e-01,  8.8353e-01,  1.0781e+00,  9.4875e-01, -3.7892e-01,\n",
      "         -7.9660e-01,  7.0468e-01,  1.8851e+00, -6.5135e-01, -2.2556e+00,\n",
      "         -4.6753e-02,  4.2502e-01, -1.1477e+00,  2.9254e-01,  2.1547e+00,\n",
      "         -8.9949e-01, -9.9709e-01, -8.6746e-01, -5.5617e-01, -2.0742e+00,\n",
      "          1.3619e+00,  7.0912e-01, -9.8750e-01,  4.6553e-01,  1.8139e+00,\n",
      "         -2.4919e-01,  1.5441e+00,  5.9062e-01, -1.3187e+00, -8.3847e-02,\n",
      "          6.3885e-01,  2.1174e+00, -6.2433e-01, -5.1274e-01,  8.3067e-01,\n",
      "          3.3549e-01,  1.8118e-01, -4.2712e-01,  3.2059e-02]])\n",
      "CrypTen bias: tensor([-0.3599])\n"
     ]
    }
   ],
   "source": [
    "#Finally, we decrypt the weights\n",
    "print(\"CrypTen weights:\", w_enc.get_plain_text())\n",
    "print(\"CrypTen bias:\", b_enc.get_plain_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.07%\n"
     ]
    }
   ],
   "source": [
    "# Let's examine our accuracy on the test data\n",
    "w_final = w_enc.get_plain_text()\n",
    "b_final = b_enc.get_plain_text()\n",
    "\n",
    "#Let's load the test data. We'll load plaintext versions and get back torch tensors, since we are testing plaintext\n",
    "test_alice = crypten.load('/tmp/alice_test.pth')\n",
    "test_bob = crypten.load('/tmp/bob_test.pth')\n",
    "test_complete = torch.cat([test_alice, test_bob], dim=2)\n",
    "test_flattened = test_complete.flatten(start_dim=1)\n",
    "targets = label_test\n",
    "\n",
    "#compute output\n",
    "output = w_final.matmul(test_flattened.t()) + b_final\n",
    "output_sign = output.sign()\n",
    "\n",
    "#compute accuracy of output\n",
    "output_target = output_sign*targets\n",
    "correct = (output_target + 1).mul(0.5).sum().float()\n",
    "accuracy = correct/targets.size(0) * 100\n",
    "print(\"Test Accuracy: %.2f%%\" % accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, Alice and Bob may only need the labels of the test data in plaintext. In this situation, we would not need to decrypt `w_enc` and `b_enc`. Instead, we could encrypt the the test data, and use the encrypted classifier (i.e., with `w_enc` and `b_enc`) to classify the encrypted test data. The labels we get will be encrypted, and only these we would need to decrypt. The trained classifier itself remains encrypted.  \n",
    "\n",
    "There is one final item to understand. As we did in the earlier tutorials, we have used `get_plain_text` to decrypt the `CrypTensors`. For this function to succeed, all the parties have to communicate their secret shares in order to carry out the decryption. Thus, the `CrypTensors` can only be decrypted if Alice and Bob agree to do so. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application 2: Data Augmentation\n",
    "\n",
    "Next, we'll show how we can use CrypTen in the <i>Data Augmentation</i> application. Here Alice and Bob each have some examples, and would like to learn a classifier over their combined examples. As before, Alice and Bob wish to keep their respective data private. \n",
    "\n",
    "The steps we take are very similar to the <i>Feature Aggregation</i> application: (a) initialize each process with its data and dummy input, (b) encrypt the data, (c) concatenate the data, and (d) learn on encrypted tensors. Indeed, the main difference comes in Step (c), where the concatenation of the `CrypTensors` is done along the batch dimension.\n",
    "\n",
    "Let's walk through the first few steps to make this clear. We'll assume that because Alice and Bob each have part of the examples, they will also have only the corresponding part of the labels. Thus, we'll encrypt the labels and combine the encrypted labels as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we'll use the mnist_utils.py script to split the public MNIST data appropriately for Alice and Bob\n",
    "%run ../examples/mnist_utils.py --option data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 Size of Alice's encrypted data:\n",
      " Examples: torch.Size([43200, 28, 28]) Labels: torch.Size([43200])\n",
      "Rank 1 Size of Bob's encrypted data:\n",
      " Examples: torch.Size([16800, 28, 28]) Labels: torch.Size([16800])\n",
      "\n",
      "Rank 0 Size of Alice's encrypted data:\n",
      " Examples: torch.Size([43200, 28, 28]) Labels: torch.Size([43200])\n",
      "Rank 0 Size of Bob's encrypted data:\n",
      " Examples: torch.Size([16800, 28, 28]) Labels: torch.Size([16800])\n",
      "\n",
      "Rank 1 Size of the combined data:\n",
      " Examples: torch.Size([60000, 28, 28]) Labels: torch.Size([60000])\n",
      "Rank 1 Combined data:\n",
      " Examples encrypted: True. Labels encrypted: True.\n",
      "Rank 0 Size of the combined data:\n",
      " Examples: torch.Size([60000, 28, 28]) Labels: torch.Size([60000])\n",
      "Rank 0 Combined data:\n",
      " Examples encrypted: True. Labels encrypted: True.\n"
     ]
    }
   ],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def construct_and_combine_encrypted_data():\n",
    "\n",
    "    #Step (a): Load each party's data into their process\n",
    "    x_alice = crypten.load('/tmp/alice_train.pth', src=0)\n",
    "    x_bob = crypten.load('/tmp/bob_train.pth', src=1)\n",
    "    \n",
    "    y_alice = crypten.load('/tmp/alice_train_labels.pth', src=0)\n",
    "    y_bob = crypten.load('/tmp/bob_train_labels.pth', src=1)\n",
    "\n",
    "        \n",
    "    #Step (b): Encrypt the data\n",
    "    x_alice_enc = crypten.cryptensor(x_alice, src=0)\n",
    "    y_alice_enc = crypten.cryptensor(y_alice, src=0)\n",
    "\n",
    "    x_bob_enc = crypten.cryptensor(x_bob, src=1)\n",
    "    y_bob_enc = crypten.cryptensor(y_bob, src=1)\n",
    "    \n",
    "    rank = comm.get().get_rank()\n",
    "    \n",
    "    #Step (c): Create the combined encrypted data\n",
    "    print(f\"Rank {rank} Size of Alice's encrypted data:\\n Examples: {x_alice_enc.size()} Labels: {y_alice_enc.size()}\") \n",
    "    print(f\"Rank {rank} Size of Bob's encrypted data:\\n Examples: {x_bob_enc.size()} Labels: {y_bob_enc.size()}\")\n",
    "    print()\n",
    "\n",
    "    #Combine the examples and labels: concatenate along batch dimension\n",
    "    x_combined_enc = crypten.cat([x_alice_enc, x_bob_enc], dim=0)\n",
    "    y_combined_enc = crypten.cat([y_alice_enc, y_bob_enc], dim=0)\n",
    "\n",
    "    print(f\"Rank {rank} Size of the combined data:\\n Examples: {x_combined_enc.size()} Labels: {y_combined_enc.size()}\")\n",
    "    print(f\"Rank {rank} Combined data:\\n Examples encrypted: {crypten.is_encrypted_tensor(x_combined_enc)}.\" + \n",
    "            f\" Labels encrypted: {crypten.is_encrypted_tensor(y_combined_enc)}.\")\n",
    "        \n",
    "z = construct_and_combine_encrypted_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step (c) contains only main difference from the <i>Feature Aggregation</i> application. Here we concatenated the data along the batch dimension (`dim 0`), while in <i>Feature Aggregation</i>, we used the feature dimension (`dim 1`). \n",
    "\n",
    "We can now train with this data exactly as we did earlier, in Step (d).\n",
    "\n",
    "This completes our tutorial on access control in CrypTen in the context of two common applications."
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "2421368098141808"
  },
  "disseminate_notebook_info": {
   "bento_version": "20190826-030256",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/shobha/fbsource/fbcode/bento/kernels/local/cryptenk/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "138739",
   "others_can_edit": true,
   "reviewers": "",
   "revision_id": "655907724900490",
   "tags": "",
   "tasks": "",
   "title": "Tutorial 3 -- Introduction to Access Control"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
